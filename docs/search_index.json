[
["index.html", "Bayesian Reproducibility Introduction", " Bayesian Reproducibility Huijia Yu May 2018 Introduction P-values have been the reason behind lack of reproducibility in scientific discoveries and especially in replicated studies and multiple testing. This problem is especially prevalent in Genome-Wide Association Studies (GWAS), where estimated effects have upward bias and often fail to replicate in validation studies. This phenomenon is known as the winner’s curse (Zöllner &amp; Pritchard, 2007). To account for this discrepancy, previous studies perform two analyses: one with all the data together, and one only using the validation site data. However, this approach is based on the underlying assumption that the association found in discovery sites is true, which is problematic for multiple testing applications such as genome-wide association studies. Furthermore, if there is a true effect, leaving out the discovery data, which could be a large portion of the total dataset, reduces power. One example of the winner’s curse in action is the analysis of the association between single nucleotide polymorphisms (SNPs) in the p53 protein, which is needed for cell growth and DNA repair, and invasive ovarian cancer. Mixed effect SNP-at-a-time analysis of 5 SNPs that were chosen for replication resulted in associations between 2 SNPs and serous invasive cancer (Joellen M. Schildkraut et al., 2009). Only one of these was strongly supported to be associated in a follow-up analysis using Multi-level Inference for SNP Association (MISA), which employs Bayesian Model Averaging and Bayes Factors for selection (Joellen M Schildkraut et al., 2010). However, most recent studies have not found evidence of association between any TP53 SNPs and cancer (C. M. Phelan et al., 2017). After a review of existing literature dealing with this issue, we propose three approaches to adress this: a fully Bayesian model, a conditional likelihood prior for discovery site data, and a bayes factor approximation to the probability of association. The performance of these methods is tested on normal simulations, and then on hierarchical simulations split into discovery and validation “sites”. All three proposed methods provide improvements over naive models. Finally, the models are used to reanalyze the p53 SNPs; none of the SNPs were found to be significant under any model. "],
["1-lit-review.html", "Chapter 1 Literature Review", " Chapter 1 Literature Review P-values have been the reason behind lack of reproducibility in scientific discoveries, causing concern and leading to proposals of new ways to define significance. Benjamin et. al have shown that the Bayes factor equivalents for commonly used p-values only correspond to “weak” evidence in the Bayes factor characterization. (Benjamin et al., 2017) They suggest reducing the p-value threshold in studies with less power, but acknowledge that hypothesis testing with thresholding is still an issue. Another approach is suggested by Selke et al , who propose two calibrations of the p-value: as the lower bound of the Bayes factor under any alternative hypothesis, and as a posterior probability of the type 1 error in a Bayesian framework (Sellke, Bayarri, &amp; Berger, 2001). This problem has become a major issue in replicated studies, an effect known as the “winner’s curse” (Zöllner &amp; Pritchard, 2007) or the Beavis effect (S. Xu, 2003). Zollner and Pritchard first define this in the context of genome-wide association scans (GWAS), which use stringent thresholds for significance, resulting in inflated effect sizes after selection, especially since these are calculated with the same data. Thus, replication studies underestimate the sample size necessary and do not have enough power to detect an effect. Zollner and Pritchard suggest a conditional-likelihood based method to address this issue. They propose a computational algorithm to maximize over the the likelihood of the parameters conditional on the significance association at level \\(\\alpha\\), which results in less biased coefficient estimates (albeit with larger variance) and sample size estimates centered at the true value.(Zöllner &amp; Pritchard, 2007) Zhong and Prentice also propose a similar method, but use a different parametrization and an asymptotic approximation instead of a computational one to find the estimators, which is more computationally efficient (Zhong &amp; Prentice, 2008). Ghosh et al. also define an approximate conditional likelihood, and propose two more estimators (other than the MLE): the mean of the (normalized) conditional likelihood, which can be interpreted as a posterior mean of the parameters under a flat prior, and a “compromise” estimator which is the average of the mean and MLE (Ghosh, Zou, &amp; Wright, 2008). The combination estimator proves to have the most stable MSE accross the range of true values for the parameters. Their approach only requires summary statistics, so they further apply it to published datasets. The results are similar for the three conditional likelihood approaches, which have been applied to other studies such as Palmer and Peter . Another method proposed to create bias-reduced estimates uses bootstrap re-sampling to correct for both the thresholding effect and the ranking effect, which is not addressed in the conditional likelihood methods because of the difficulty of specifying joint likelihoods for correlated variables (Sun et al., 2011). By using a sample-split approach, the detection and estimation datasets can be virtually independent. This is repeated multiple times in order to reduce variance in the results. The main drawback of this approach is its computational intensity. Several authors have also proposed shrinkage-based methods in the effect detection step. Bacanu and Kendler use a soft threshold method to scale statistics such that their sum of squares do not overestimate the true mean and then find “suggestive” signals in a GWAS context by setting a threshold. This method does not address the winner’s curse directly, but provides a subset of the genome which can be futher analyzed or used in future studies (Bacanu &amp; Kendler, 2013). Bigdelli et al. propose shrinking coefficient estimates by drawing a comparison between “winner’s curse adjustments” for effect sizes and multiple testing approaches for p-values, since both are used on the tail of their respective distributions. Their method transforms False Discovery Rate (FDR) adjusted p-values into the corresponding Z-score and uses that as the estimator.(Bigdeli et al., 2016) Both Bigdelli and Bacanu assume the data is normally distributed. Storey and Tibshirani , on the other hand, propose to adjust the value used for significance testing rather than the coefficients, choosing the FDR value as an alternative to the p-value. Multiple Bayesian methods have also been proposed: Xu et al use a Bayesian approach to a logistic regression, selecting a spike and slab prior for the mean and an inverse gamma prior for the variance. (L. Xu, Craiu, &amp; Sun, 2011) A beta prior for the proportion of each component in the prior, and the hyperparameters were estimated empirically. They also propose a Bayesian Model Average approach, which they recommend for instances with little prior information. Their results show that the Bayesian models had smaller variance than conditional likelihood methods, but still do not address the “ranking effect” from Sun (Sun et al., 2011), or implement a fully Bayesian approach because of the dependence on the threshold \\(\\alpha\\). Ferguson et al propose an Empirical Bayes approach, which estimate the prior density distribution with the data (Ferguson, Cho, Yang, &amp; Zhao, 2013). This is a nonparametric estimate, but still depends on other specifications such as the number of bins, type of splines, etc. Using the empirical prior, the posterior is then calculated, from which the estimate and pseudo-Bayesian credible intervals are derived by considering the 5% and 95% points. This method resulted in better estimates in the higher density regions, but performed worse than conditional likelihood methods on the tails. Thus, the authors propose a combined method, which calculates both the empirical Bayes and the conditional likelihood confidence intervals, and picks the shortest one. One possible problem with this approach is the use of non-HPD intervals, which could change the tail behavior. Jiang and Yu apply the Bayesian framework to power calculations specifically, defining “Bayesian power” as the marginal probability of finding significance in a replicated study given the original and the data. They also use a spike and slab prior, but estimate the hyperparameters empirically. The resulting power estimators are improved, but lead to downwards bias in the effect size.(Jiang &amp; Yu, 2016) "],
["2-models.html", "Chapter 2 Models 2.1 Fully Bayesian Model 2.2 Conditional Likelihood 2.3 Bayes Factor Model 2.4 Methods", " Chapter 2 Models We propose three different Bayesian approaches: A fully Bayesian mixed effects hierarchical model that can jointly perform significance testing and effect estimation. By combining the testing and estimate steps, we can overcome the winner’s curse and account for the uncertainty that arises when selecting an SNP. A conditional likelihood model that can take into account the probability of finding a significant result in the discovery sites when estimating effect size. A bayes factor based model that uses the bayes factor from the discovery sites (which is more reliable than the p-value, as discussed previously) to quantify the uncertainty of the significant result. While the first approach is truly Bayesian and requires all the data, the second and third can be used as long as the sufficient statistics (MLE, SE, p-value, \\(\\alpha\\)) are available. 2.1 Fully Bayesian Model Let \\(\\delta_a(x)\\) be the Dirac delta function: \\(\\delta_a(x) = 1\\) for \\(x = a\\) and \\(\\delta_a(x)=0\\) otherwise. The test for significance is \\(H_0: \\mu = 0\\), \\(H_1: \\mu \\neq 0\\), where \\(\\mu\\) is the mean effect size. Then, \\(P( \\mu|H_0) = \\delta_0 ( \\mu )\\), and \\(P( \\mu|H_1) = N(0,1)\\) or some other diffuse prior. We can define a hyperparameter \\(\\xi\\) such that \\(P(H_1) = \\xi\\). This gives rise to a latent variable \\(\\iota\\) drawn from a bernoulli(\\(\\xi\\)), which is then used to parametrize the distribution of \\(\\mu\\). If \\(\\iota= 0\\), \\(\\mu=0\\). For a hierarchical framework, site means \\(\\beta_{j}|\\iota=1 \\sim N(\\mu, \\sigma^2)\\), and \\(\\beta_{j}|\\iota = 0 = 0\\). In this case the prior for \\(\\mu\\) is a mixture of a point mass at zero and a Cauchy(0,1). The prior for \\(\\sigma\\) was a truncated Cauchy(0,1), with support only on the positive real line. This choice of priors is based on simulation results. The prior for \\(\\xi\\), the probability of the alternative, was a Beta(.5,.5), which has a U-shape so that it favors 0 or 1 more heavily than the values between them. The complete model is as follows: \\[\\beta_{j}|\\iota = 1 \\sim \\textsf{N}(\\mu, \\sigma^{2}) \\\\ \\mu|\\iota=1\\sim \\textsf{Cauchy}(0,0.1)\\\\ \\mu, \\beta_{j}|\\iota = 0 =0\\\\ \\sigma\\sim \\textsf{Cauchy}(0,1), \\sigma\\geq 0\\\\ \\iota \\sim \\textsf{Bernoulli}(\\xi)\\\\ \\xi \\sim \\textsf{Beta}(1/2, 1/2)\\] There is no difference between discovery and validation sites in the Bayesian framework. Even considering them separately, one could consider the posterior distributions of the parameters given only discovery site data as the priors given the validation data, which would result in exactly the same results. 2.2 Conditional Likelihood In this case, the results from the discovery sites are used as a prior for the validation data analysis, which is why only the sufficient statistics are needed. Given the discovery sites’ MLE and SE, we can use the CLT and definition of MLE to state that \\(MLE_i \\sim N(\\beta_i, SE_i)\\). Let \\(B\\) indicate that the data is significant at the level \\(\\alpha\\). The conditional likelihood \\(L(\\mu | B) = \\frac{P(Y| \\mu)P(B| Y,\\mu)}{P(B|\\mu)} = \\frac{P(Y| \\mu)}{\\int_{\\textsf{significant Y}} P(t| \\mu) dt }\\). Conditioning on finding a significant estimate, \\(P(MLE_i | B) = \\frac{\\phi(MLE_i, \\beta_i, SE_i)}{\\Phi(-q_i, \\beta_i, SE_i)+1-\\Phi(q_i, \\beta_i, SE_i)}\\), where \\(\\phi(x, \\beta_i, \\sigma)\\) is the pdf of a normal distribution with mean \\(\\beta_i\\) and variance \\(\\sigma^2\\), and \\(\\Phi(x, \\beta_i, \\sigma)\\) is the cdf of the same distribution. The value of \\(q_i\\) is \\(\\Phi^{-1}(1-\\frac{\\alpha}{2}, 0 ,SE_i)\\), where \\(\\alpha\\) is the power of the test (i.e. p-values that are smaller than \\(\\alpha\\) are considered significant). This is cutoff for an MLE value to be considered significant. Let the conditional likelihood of \\(MLE_i\\) be denoted as \\(\\textsf{CL}(\\beta_i,SE_i, q_i)\\). We can see that as \\(\\alpha\\) decreases (i.e. the tests are more strict), the likelihood becomes more skewed towards 0. In the hierarchical setting, we used the random effect conditional likelihood model as a “prior” for \\(\\beta_{ j}, j\\in \\text{discovery}\\), and then use this as the prior \\(P(\\beta | \\text{discovery})\\) for the model with the validation data. The updated model is: \\[\\beta_{j} \\sim \\textsf{N}(\\mu, \\sigma^{2p53}) , j \\in \\text{validation}\\\\ MLE_{j} \\sim \\textsf{CL}(\\beta_{j},SE_{j}, q_j) , j \\in \\text{discovery}\\\\ \\sigma\\sim \\textsf{Cauchy}(0,1), \\sigma\\geq 0\\] Note that the selection uncertainty is somewhat accounted for through the conditional likelihood, but there is no measure of this uncertainty. By using the discovery MLEs, we are already assuming that there is a nonzero effect. 2.3 Bayes Factor Model The discovery data can be used not only in estimating the distribution of the size of a preestablished effect (\\(\\mu\\)), but in estimating the distribution of the probability of the effect itself (\\(\\xi = P(H_1)\\)). To make this model easily generalizable, we use the upper bound on the Bayes Factor \\(BF = \\frac{L(\\bar Y | H_1)}{L(\\bar Y | H_0)} \\leq \\frac{1}{-e p log(p)}\\), where \\(p\\) is the p-value from the discovery data (Sellke et al., 2001). This is a “best-case scenario” of how much evidence there is from data given a particular p-value. Since this value is fixed given the discovery data, we can then consider the “posterior”&quot; probability of true association \\(\\xi\\) given the discovery p-value as a transformation of \\(\\xi\\), which is parametrized with prior Beta(.5,.5). Let \\(o\\) be the prior odds \\(\\frac{1-\\xi}{\\xi}\\). The transformation \\(\\xi&#39; = \\frac{P(H_1)*L(Y|H_1)}{P(H_0)*L(Y|H_0)+P(H_1)*L(Y|H_1)} = \\frac{o*BF}{1+o*BF}\\). Then \\(\\xi&#39;\\) can be used in the overall model with the validation data. In this case, the discovery data is not used at all to estimate the effect sizes, but it will have an effect on the amount of zero-valued global effects sampled because it skews the distribution to the right. Note that for small p-values, this can be very extreme. For a GWAS p-value \\(p = 10^{-7}\\) and \\(\\xi \\sim \\textsf{Beta}(.5,.5)\\) , \\(P( \\xi&#39; \\leq 0.5) =\\) \\(4.74\\times 10^{-11}\\) . Due to this, we use a flatter prior: \\(\\xi \\sim \\textsf{Beta}(.9,.9)\\) . Then \\(P( \\xi&#39; \\leq 0.5) =\\) \\(1.24\\times 10^{-6}\\). This is extremely sensitive to the choice of prior as well as to the p-value. While the skew is appropriate for this particular prior, it would not necessarily make sense with a flat or informative prior. 2.4 Methods Models were fit using R2jags and in the simpler cases, with original Metropolis Hastings algorithms. To specify distributions that are not part of the R2jags library, such as the conditional likelihood, we use the “ones trick”, which is implement by creating artificial observations of a Bernoulli variable. Consider a prior for \\(\\theta\\) that is proportional to \\(\\pi(\\theta)\\). If we set that bernoulli variable “ones” is equal to 1 with probability \\(\\pi(\\theta)\\), create an observation “ones”\\(= 1\\), and set a uniform prior for \\(\\theta\\), then we are effectively creating a “posterior” for theta that is proportional to \\(\\pi(\\theta)\\) as intended. Each JAGS model was run with the default settings: 3 chains, 2000 iterations, and 1000 burn-in samples. JAGS model functions for the hierarchical simulations can be found in the supplement. All computed credible intervals are HPD (highest posterior density) intervals. A 95% HPD interval is the 95% of the sampled values with the highest density. HPD intervals are guaranteed to be the shortest intervals for that scale (they are not scale-invariant), and can give more reasonable answers for multimodal distributions than quantile-based intervals because they can be disjoint. Point estimates were calculated used the posterior median, so that these estimates would be invariant to transformations (e.g. log). "],
["3-normal-simulation-study.html", "Chapter 3 Normal Simulation Study 3.1 Data Generation 3.2 Conditional Likelihood 3.3 Posterior Distribution 3.4 Results", " Chapter 3 Normal Simulation Study 3.1 Data Generation To test the hypothesis \\(H_0: \\mu = 0\\) versus \\(H_1: \\mu \\neq 0\\), a fixed proportion (set at 0.5) of null vs. alternative hypotheses are generated. For each hypothesis \\(H_i\\), let \\(\\mu_i = 0\\) in the null scenario and \\(\\mu_i \\sim \\textsf{N}(0,1)\\) in the alternative. The data \\(Y_i\\) is generated from a normal distribution with mean \\(\\mu_i\\) and known variance 1, with sample size 100. If \\(Y_i\\) is not significant at \\(\\alpha = .05\\), it is sampled again from the same distribution until the sufficient statistic is significant. This is done in order to properly compare the Bayesian approach with the frequentist one, which is conditional on the data being significant. 3.2 Conditional Likelihood In this case, the sufficient statistic can be used to simplify the conditional likelihood: since \\(\\bar Y \\sim \\textsf{N}(\\mu, \\sigma^2/n)\\), the conditional likelihood for the simulated data is equivalent to \\(L(\\mu|\\bar Y) = \\frac{\\phi((\\bar Y-\\mu)/\\sqrt{n})}{\\Phi(Z_{\\alpha/2}-\\mu\\sqrt{n})+\\Phi(Z_{1-\\alpha/2}-\\mu\\sqrt{n})}\\). Since this likelihood is difficult to integrate analytically, the confidence intervals were estimated by treating the conditional likelihood as if it were a posterior distribution with an improper prior \\(\\pi(\\mu) = 1\\), and obtaining the HPD region covering 95%. Sampling was done through a Metropolis-Hastings algorithm. 3.3 Posterior Distribution In the Bayesian case, the prior was set to the mixture model \\(\\pi(\\mu|\\xi) = (1-\\xi ) \\delta_0(\\mu)+ \\xi\\phi(\\mu)\\). In this case, \\(\\xi = 0.5\\) is a constant. Note that this is also the true data generating model. The marginal posterior distribution is \\(P(\\mu | Y ) = P(H_0|Y)P(\\mu|Y, H_0) + P(H_1|Y)P(\\mu|Y, H_1)\\). The separate posteriors for \\(\\mu\\) are: \\(P(\\mu|Y, H_0) = \\delta_0(\\mu)\\), \\(P(\\mu|Y, H_1) \\sim \\textsf{N}(\\frac{n}{n+1}\\bar Y, \\frac{1}{n+1})\\). The posterior for the alternative hypothesis can be calculated using its bayes factor, BF and the prior odds, \\(\\pi = \\frac{(1-\\xi)}{\\xi}\\): \\(P(H_1| Y ) = \\frac{\\pi BF}{1+\\pi BF}\\). For this example, the prior odds are 1 (because the probability of \\(H_1 = \\xi = 0.5\\)). The bayes factor \\(BF = \\frac{L(\\bar Y | H_1)}{L(\\bar Y | H_0)} = \\sqrt{n+1}* e^{\\frac{n^2}{2(n+1)}(\\bar Y)^2}\\). This result comes from the fact that the marginal likelihood \\(L(\\bar Y | H_1) \\sim \\textsf{N}(0, \\frac{n}{n+1})\\). Putting these pieces together results in the marginal posterior for \\(\\mu\\), which can be used to generate samples to calculate credible intervals. 3.4 Results 3.4.1 Estimators No id variables; using all as measure variables RMSE bayes 3.810038 bayes.median 4.328932 bayes.mode 4.328932 cond.mean 3.931742 cond.mode 3.868903 naive 4.986346 The conditional likelihood mode (i.e. MLE) has the smallest bias (absolute error) for \\(\\mu\\) out of the frequentist estimators, while the bayesian median and mode (which end up being the same) the smallest bias in the bayesian framework. The RMSE for the bayesian estimator (mean of the posterior) is the lowest, followed by the conditional mean and mode. 3.4.2 Credible and Confidence Intervals The lines mark the \\(y = x\\) line, and the length of naive confidence intervals (which are constant for fixed number of samples) on the x and y axes. The largest values for the significant statistic also correspond to the largest intervals in both cases. Note that the conditional likelihood confidence intervals are almost always larger than the credible intervals, but still mostly smaller than the naive ones. 3.4.3 Coverage The marginal coverage of the (frequentist conditional likelihood) confidence interval C, \\(P(\\mu \\in C|Y) = P(\\mu \\in C|H_0) P(H_0|Y)+P(\\mu \\in C|H_1) P(H_1|Y)\\) will be significantly higher than .95 for the cases in which \\(0 \\in C\\), since \\(P(\\mu \\in C|H_1) =0.95\\) by definition, and \\(P(\\mu \\in C|H_0) = I_{0 \\in C}\\). In this experiment, the expected coverage is \\(0.98\\) for intervals with 0, and only \\(0.38\\) for those that do not contain 0. However, conditioning on the alternative hypothesis does not lead to an empirical coverage of 95%. We can see that both methods are still significantly better than the naive one. Table 3.1: Empirical Coverage for 95% Confidence/Credible Intervals Unconditional Coverage Coverage Conditional on H0 Coverage Conditional on H1 naive 0.6572238 0.6235294 0.7448980 conditional 0.8810198 1.0000000 0.5714286 bayesian 0.8824363 1.0000000 0.5765306 3.4.4 Hypothesis Rejection Due to the nature of p-values, an \\(\\alpha = 0.05\\) corresponds to a posterior probability \\(P(H_1 | Y )\\) of only \\(0.4\\) for \\(N = 100\\). This means that the 95% credible interval for \\(\\mu| Y\\) will contain 0 every time. In terms of hypothesis testing, if we consider the strategy of rejecting the null when the interval does not contain 0, this level for \\(\\alpha\\) leads to no rejections. Table 3.2: Naive method Do not reject null Reject null 0 0.2719547 0.4504249 1 0.1373938 0.1402266 Table 3.3: Conditional Likelihood Method Do not reject null 0 0.7223796 1 0.2776204 Table 3.4: Bayesian Mixture Model Do not reject null 0 0.7223796 1 0.2776204 Despite never rejecting the null, the conditional likelihood and the bayesian methods both perform better than the naive one in terms of “predicting” accurately. The naive method is especially problematic in that it has a higher Type 1 error (false positives) than true positives OR true negatives in the region of the data. "],
["4-hierarchical-simulations.html", "Chapter 4 Hierarchical Simulations 4.1 Data Generation Procedure 4.2 Results 4.3 Sensitivity of Conditional Likelihood Method to Changes in \\(\\alpha\\)", " Chapter 4 Hierarchical Simulations 4.1 Data Generation Procedure The data are generated from a hierarchical (i.e. mixed effect) logistic model as discribed in the data section: if truly associated, \\(\\mu\\) and \\(\\sigma^2\\) have (fixed) nonzero values;\\(\\beta_j \\sim \\textsf{N}(\\mu, \\sigma^2)\\). Otherwise, \\(\\mu=\\beta_j = 0, \\forall j\\). The observed data \\(Y_{ij}\\) is binary, and has \\(P(Y_{ij}=1| \\beta_j) = \\frac{e^{\\beta_j}}{1+e^{\\beta_j}}\\). The \\(j\\) index corresponds to the “group” to which the observation belongs. \\[P(Y_{ij}=1) = \\textsf{logit}^{-1}(\\beta_{j})\\\\ \\beta_{j}|H_1 \\sim \\textsf{N}(\\mu, \\sigma^{2})\\\\ \\beta_{j}|H_0 = 0\\] To try to keep this simulation as close to the real data as possible, a preliminary logistic regression with random slope and random p53 coefficient by site was run. This led to the values of \\(\\mu =0.203 , \\sigma^2 = 0.003\\). The value of \\(\\mu\\) remained fixed through all the simulations, but different values of \\(\\sigma\\) were used to test the sensitivity of the models: \\(\\sigma\\), \\(\\sigma/2\\), \\(2\\sigma\\), and \\(4\\sigma\\). The number of sites was set to 7, since results using 30 sites were almost identical. Each site had 1000 observations. A total of 100 simulated datasets was created each time. 100 datasets were also simulated under the null hypothesis. They were fit with the models described above, and resulted in reasonable estimates. 4.1.1 Finding “Discovery” Sites In this simulation study, a logistic regression with fixed effects for sites was conducted to find the site with the smallest p-value less than \\(\\alpha\\). If no sites matched this description, the data was resampled until at least one site was viable. The maximum likelihood estimate of this effect and its variance were added as data for the conditional likelihood model, and the p-value was added to the bayes factor approximation model. The observations for this site were then taken out of the data. 4.2 Results 4.2.1 \\(\\mu\\) 4.2.1.1 RMSE As expected, the models performed more poorly as \\(\\sigma\\) increased. Out of the three proposed models (compared with the original), the fully bayesian and BF approximation models performed best when there was no true effect (since they were the only ones that that had this option). However, there were some simulated datasets where the bayes factor model estimate was actually nonzero and quite large, which suggests that it is not nearly as reliable as the bayesian model. At small variances (s/2, s, 2s), the original and bayesian models outperform the others. This is not surprising since the other models only have access to \\(\\frac{6}{7}\\) of the data. The Bayesian model actually has a slightly higher lower RMSE than the other models when there is a true association. Table 4.1: RMSE of \\(\\mu\\) mu=0 s/2 s 2s 4s Bayesian 0.0000000 0.0266044 0.0316132 0.0525072 0.1036748 Cond Likelihood 0.0205718 0.0269924 0.0334522 0.0543457 0.1087974 BF Approx 0.0136705 0.0291403 0.0348389 0.0579472 0.1154215 Original 0.0246810 0.0259643 0.0309745 0.0499914 0.1032853 4.2.1.2 Coverage The conditional likelihood and BF approximation models are the most conservative, with the intervals covering 0 more times than the others for large values of \\(\\sigma\\). All models have very high coverage in general. The Bayesian model had the shortest intervals, and the original model had the largest. Thus, even though the coverage and RMSE are around the same, the new models are preferable to the original. This does not apply to the simulation with \\(\\sigma=4s\\), because \\(4s&gt;\\mu\\), which leads to more negative site effects. Thus, it makes sense for models to have wider credible intervals for these simulations. 4.2.2 \\(\\xi\\) While one would expect the probability of being associated (\\(\\xi\\)) to also increase with \\(\\sigma\\), this was not true for either the fully bayesian model nor the bayes factor approximation one, both of which had consistent posterior estimates of \\(\\xi\\). Similarly, the proportion of nonzero \\(\\mu\\) samples from the posterior (this is the same as the proportion of times the latent variable \\(i = 1\\)), was almost 1 for the truly associated cases, and close to zero for true null. One thing to consider is that under the null hypothesis, the variance across sites would actually be zero, which is why the models identified the association so decisively. The Bayes Factor approximation model has much larger median \\(\\xi\\) and \\(\\iota\\) because the mass of the distribution of \\(\\xi\\) is shifted towards 1. Thus, even though the median of \\(\\iota\\) is quite low (and rhe median of \\(\\mu\\) is 0), the median of \\(\\xi\\) is greater than \\(0.5\\). Table 4.2: Average of Posterior Median \\(\\xi\\) mu=0 s/2 s 2s 4s Bayesian 0.1885539 0.8334633 0.8331898 0.8311593 0.8357240 BF Approx 0.7873461 0.9914407 0.9920261 0.9924780 0.9992027 Table 4.3: Average of Mean of \\(\\iota\\) mu=0 s/2 s 2s 4s Bayesian 0.0963794 0.9923888 0.9917315 0.9910799 0.9947298 BF Approx 0.2796165 1.0000000 1.0000000 0.9896530 1.0000000 4.3 Sensitivity of Conditional Likelihood Method to Changes in \\(\\alpha\\) 4.3.1 With Random Effects The conditional likelihood method with random effects is robust to changes in the level \\(\\alpha\\). To test this, we consider 5 different levels: \\(0.05, 0.01, 0.005, 0.001, 10^{-7}\\). 100 datasets were sampled, for which at least one location was significant at the smallest \\(\\alpha\\) level. The conditional likelihood model with random effects and without was then fitted for each level. Table 4.4: RMSE of \\(\\mu\\) 0 s 4s 0.05 0.0188881 0.0386697 0.1005714 0.01 0.0198722 0.0402381 0.0900666 0.005 0.0188018 0.0400860 0.0986015 0.001 0.0191543 0.0384415 0.0976565 1e-07 NA 0.0385995 0.0969108 This model shows little difference across levels. 4.3.2 No Random Effects A simpler model than the one proposed has no random effects: \\(\\beta_j\\sim \\textsf{N}(\\mu, \\sigma^2)\\) and \\(MLE_i \\sim \\textsf{N}(\\mu, SE_i)\\). Table 4.5: RMSE of \\(\\mu\\) 0 s 4s 0.05 0.0192390 0.0373407 0.1052692 0.01 0.0193837 0.0364925 0.0929499 0.005 0.0172387 0.0375955 0.0984933 0.001 0.0187290 0.0392056 0.0955200 1e-07 NA 0.0395267 0.0992272 Removing the uncertainty for site-specific means leads to a reduction in the length of the credible intervals for \\(\\mu\\). The interval length in the random effect model was larger for simulations with large variance between sites (\\(\\sigma^2 = 4s\\)) than for simulations with small variance (\\(\\sigma^2 = s\\)), which appropriately captures the uncertainty of the estimate for \\(\\mu\\). However, the model with no random effects creates credible intervals that are the same length for simulations with \\(\\sigma^2 = 4s\\) and \\(\\sigma^2 = s\\). The RMSE is very similar for the two models for each \\(\\sigma^2\\), and neither model’s credible interval length or median estimate changes with \\(\\alpha\\). "],
["5-analysis-of-tp53.html", "Chapter 5 Analysis of TP53 5.1 Data 5.2 Models 5.3 Results", " Chapter 5 Analysis of TP53 5.1 Data 5.1.1 Discovery Sites Three independent discovery studies focused on TP53 polymorphisms and risk of ovarian cancer: the North Carolina Ovarian Cancer Study (NCOCS), the Mayo Clinic Case-Control Study (MAYO), and the Polish Ovarian Cancer Study (POCS). These were restricted to non-Hispanic white women with newly diagnosed, histologically confirmed, primary invasive epithelial ovarian cancer and to non-Hispanic white controls. 23 SNPs were genotyped in total, with some overlap between sites. 5.1.2 Replication Sites Ten other sites contributed data: the Australian Ovarian Cancer Study (AOCS) and the Australian Cancer Study (ACS) presented together as AUS, the Family Registry for Ovarian Cancer (FROC, presented as STA), the Hawaiian Ovarian Cancer Study (HAW), the Malignant Ovarian Cancer Study Denmark (MALOVA), the New England Case-Control Study (NEC), the Nurses’ Health Study (NHS), SEARCH Cambridge (SEA), the Los Angeles County Case-Control Study of Ovarian Cancer (LAC-CCOC, presented here as USC), the University of California at Irvine study (UCI), and the United Kingdom Ovarian Cancer Population Study (UKOPS, presented here as UKO). The combined data set (discovery and replication) comprised 5,206 white, non-Hispanic invasive epithelial ovarian cancer cases, of which 2,829 were classified as serous invasive ovarian cancer, and 8,790 white non-Hispanic controls. Analysis was restricted to white, non-Hispanic invasive serous ovarian cancer cases and white, non-Hispanic controls. 5.2 Models Each model adjusts for study site, reference age, and personal history of breast cancer. History of breast cancer is treated as a fixed effect, and the rest of the covariates’s coefficients are treated as normally distributed random effects. \\[P(Y_{ij}=1) = \\textsf{logit}^{-1}(\\beta^{site}_{ j}+ \\beta^{p53}_{j}*p53_{ij}+ \\beta^{age}_{j}*age_{ij}+\\beta^{BC}* BC_{ij})\\] \\[\\beta^{site}_{ j}\\sim \\textsf{N}(\\mu^{site}, \\sigma^{2site})\\] \\[\\beta^{age}_{j}\\sim \\textsf{N}(\\mu^{age}, \\sigma^{2age})\\] \\[\\beta^{BC}, \\mu^{age},\\mu^{site}\\sim \\textsf{N}(0,0.1)\\] \\[\\sigma^{age},\\sigma^{site} \\sim \\textsf{invGamma}(1, 0.05)\\] The p53 variable’s site-specific log OR priors were defined using the models described previously. The fully Bayesian model was fit jointly as well as marginally. Since the results were very similar, the marginal models were used for computational efficiency and clarity of interpretation. Results from the joint analysis can be found in the supplement. 5.3 Results Warning: Removed 17 rows containing missing values (geom_linerange). The new models shrunk estimates towards 0 for all SNPs. The BF approximation model set all point estimates to zero, as did the fully Bayesian model with the exception of one. This is the same SNP (rs2287498n) that was detected as significant in the MISA analysis (Joellen M Schildkraut et al., 2010). However, the credible interval contains 0, so there is not enough evidence for association. Similarly, although the conditional likelihood model does not set any estimates to zero, all its credible intervals contain zero. The original model has the longest credible intervals for all SNPs except rs2287498n, for which both the Bayesian and Bayes Factor models have large credible intervals, indicating more uncertainty than the original model. Below are the full tables of OR by site and SNP. Table 5.1: rs9894946n original estimate original CI bayesian estimate bayesian CI CL estimate CL CI BF estimate BF CI AUS 0.961 0.724 - 1.22 1 1 - 1 0.98 0.757 - 1.203 1 0.95 - 1.001 HAW 0.924 0.422 - 1.503 1 1 - 1 0.979 0.529 - 1.319 1 0.946 - 1.051 MAY 0.948 0.691 - 1.219 1 1 - 1 0.977 0.729 - 1.19 1 0.964 - 1.031 POL 1.475 0.909 - 2.148 1 1 - 1 1.068 0.91 - 1.475 NA - Overall 1.053 0.41 - 1.872 1 1 - 1 1.003 0.65 - 1 , 1 - 1.341 1 0.958 - 1.024 Table 5.1: rs12951053n original estimate original CI bayesian estimate bayesian CI CL estimate CL CI BF estimate BF CI AUS 1.256 1.022 - 1.522 1 0.965 - 1.425 1.151 0.979 - 1.407 1 0.961 - 1.296 HAW 1.195 0.814 - 1 , 1.001 - 1.542 1 0.892 - 1.428 1.096 0.842 - 1.36 1 0.933 - 1.258 MAL 1.189 0.898 - 1 , 1.001 - 1.445 1 0.928 - 1.366 1.113 0.93 - 1 , 1.001 - 1.348 1 0.947 - 1.248 MAY 1.22 0.94 - 0.998 , 1.001 - 1.541 1 0.931 - 1.404 1.118 0.934 - 1.36 NA - NCO 1.275 1.026 - 1.654 1 0.956 - 1.515 1.066 0.78 - 1 , 1 - 1.291 NA - NEC 1.209 0.974 - 1.473 1 0.934 - 1.356 1.073 0.855 - 1.248 1 0.941 - 1.263 NHS 1.131 0.629 - 1 , 1 - 1.435 1 0.781 - 1.35 1.104 0.909 - 1.37 1 0.925 - 1.249 POL 1.187 0.891 - 0.999 , 1 - 1.464 1 0.915 - 1.363 1.102 0.896 - 1.348 NA - SEA 1.121 0.827 - 1.345 1 0.904 - 1.274 1.117 0.913 - 1 , 1 - 1.406 1 0.939 - 1.218 STA 1.191 0.876 - 0.999 , 1 - 1.481 1 0.926 - 1.374 1.121 0.923 - 1.357 1 0.943 - 1.255 UCI 1.184 0.866 - 1 , 1 - 1.446 1 0.909 - 1.376 1.079 0.932 - 1 , 1 - 1.25 1 0.935 - 1.254 UKO 1.217 0.909 - 1.567 1 0.922 - 1.432 1.099 0.956 - 1.286 1 0.942 - 1.268 USC 1.221 0.937 - 1 , 1.001 - 1.51 1 0.94 - 1.369 1.065 0.926 - 1 , 1 - 1.23 1 0.973 - 1.274 Overall 1.193 1.013 - 1.369 1 0.976 - 1.295 1.103 0.984 - 1.216 1 0.959 - 1.073 , 1.11 - 1.242 Table 5.1: rs1625895n original estimate original CI bayesian estimate bayesian CI CL estimate CL CI BF estimate BF CI AUS 1.015 0.819 - 1 , 1 - 1.193 1 1 - 1 1.004 0.805 - 1 , 1 - 1.166 1 1 - 1 HAW 1.023 0.748 - 1.283 1 1 - 1 1.013 0.729 - 0.999 , 1 - 1.266 1 1 - 1 MAL 1.051 0.848 - 1.273 1 1 - 1 1.041 0.851 - 1.253 1 1 - 1 MAY 1.049 0.847 - 1.267 1 1 - 1 1.038 0.841 - 1.255 1 1 - 1 NCO 0.997 0.803 - 1 , 1 - 1.181 1 1 - 1 0.998 0.797 - 1.152 1 1 - 1 POL 1.08 0.887 - 1.463 1 1 - 1 1.039 0.862 - 1.251 NA - SEA 1.049 0.855 - 1 , 1 - 1.247 1 1 - 1 1.021 0.79 - 1.228 1 1 - 1 STA 1.032 0.82 - 1.279 1 1 - 1 1.047 0.9 - 1 , 1 - 1.23 1 1 - 1 Overall 1.039 0.875 - 1 , 1 - 1.188 1 1 - 1 1.025 0.879 - 1.161 1 1 - 1 Table 5.1: rs1042522n original estimate original CI bayesian estimate bayesian CI CL estimate CL CI BF estimate BF CI AUS 1.043 0.824 - 1.205 1 1 - 1 1.028 0.837 - 1.154 1 1 - 1 HAW 1.059 0.751 - 1 , 1 - 1.286 1 1 - 1 1.035 0.82 - 1 , 1 - 1.234 1 1 - 1 MAL 1.096 0.94 - 1.277 1 1 - 1 1.051 0.918 - 1.204 1 1 - 1 MAY 1.109 0.939 - 1 , 1.001 - 1.322 1 1 - 1 1.059 0.934 - 1.256 1 1 - 1 NCO 1.068 0.88 - 1.244 1 1 - 1 1.039 0.897 - 1.198 1 1 - 1 POL 1.133 0.936 - 1 , 1 - 1.418 1 1 - 1 1.038 0.895 - 1 , 1 - 1.204 NA - SEA 1.065 0.879 - 1.232 1 1 - 1 1.038 0.879 - 1.204 1 1 - 1 STA 1.072 0.867 - 1 , 1 - 1.275 1 1 - 1 1.043 0.934 - 1.18 1 1 - 1 Overall 1.08 0.933 - 1.22 1 1 - 1 1.041 0.928 - 1.153 1 1 - 1 Table 5.1: rs2287498n original estimate original CI bayesian estimate bayesian CI CL estimate CL CI BF estimate BF CI HAW 1.267 0.907 - 1.711 1.193 0.929 - 1.621 1.119 0.886 - 1.495 1 0.845 - 1.567 MAL 1.251 1.004 - 1.525 1.188 0.945 - 1.5 1.139 0.928 - 1.406 1 0.937 - 1.435 MAY 1.335 1.047 - 1.803 1.261 0.963 - 1.674 1.115 0.904 - 1.383 NA - NCO 1.334 1.105 - 1.702 1.282 0.994 - 1.045 , 1.071 - 1.635 1.135 0.91 - 1.491 NA - POL 1.259 0.95 - 1.595 1.189 0.941 - 1.556 1.128 0.907 - 1.444 NA - SEA 1.218 0.929 - 1.473 1.137 0.913 - 1.49 1.225 0.974 - 1.666 1 0.926 - 1.398 STA 1.265 0.972 - 1.653 1.199 0.943 - 1.556 1.115 0.938 - 1 , 1 - 1.386 1 0.915 - 1.496 UKO 1.257 0.931 - 0.999 , 1.002 - 1.6 1.18 0.928 - 1.555 1.116 0.966 - 1.377 1 0.905 - 1.476 USC 1.366 1.087 - 1.804 1.306 0.997 - 1.046 , 1.095 - 1.73 1.087 0.909 - 1 , 1 - 1.309 1 0.95 - 1.689 Overall 1.282 1.09 - 1.515 1.236 0.977 - 1.534 1.138 0.992 - 1.333 1 0.953 - 1.433 "],
["6-conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion We propose three models to deal with the winner’s curse and reduce the selection effect in repeated studies. The fully Bayesian model uses all the data to make inference and uses a binary latent variable to model the true association. This is equivalent to a spike and slab prior, or to model averaging with two possible models: the null and the alternative. The conditional likelihood method uses the likelihood of the estimate conditional on being significant. This is a frequentist approach to selection bias, and it depends on the significance test level as well as the effect estimate and standard error. The conditional likelihood is then used as a prior in the (Bayesian) validation analysis. The bayes factor approximation method uses an upper bound on the bayes factor that is only dependent on the p-value to calculate a “best-case scenario” posterior probability of the alternative hypothesis. The distribution that arises from this transformation is used as the prior of the association probability in the validation analysis. This approach also has a frequentist component, since p-values are used, but is a step towards the Bayesian framework since the only function of the p-value is to approximate the Bayes Factor. One clear advantage of the fully Bayesian model is that it can perform testing and estimation simultaneously. This means all the data is used once, which is why the credible intervals are smaller and the RMSE is lower in the simulations. However, it is not always feasible to implement if the discovery data is unavailable. Furthermore, using Bayesian methods does not guarantee bias correction and can lead to paradoxes because it does not account for selection especially for conjugate priors in multivariate inference (Dawid, 1994). The conditional likelihood and Bayes Factor approximation methods can be used in follow-up studies even when the discovery data is not publicly available. Both provide significant improvements over the naive method. The bayes factor model has a quasi-testing feature since it accounts for the probability of a true association, but is very sensitive to the p-value as well as the choice of prior. Although the conditional likelihood itself is dependent on the significance test, this prior is only used for the discovery sites. Under a hierarchical model, the global effect is actually unaffected by the \\(\\alpha\\) level. This is extremely useful because discoveries that do not have are not significant at the \\(10^{-7}\\) level can still be used without affecting the results. The normal distribution is used in the simulation and real experiment studies presented in this project, which simplifies the integration over all significant events. However, this integration might not be computationally feasible for other distributions. "],
["7-supplementary-material.html", "Chapter 7 Supplementary Material 7.1 JAGS Models 7.2 Joint Analysis of TP53 SNPs", " Chapter 7 Supplementary Material 7.1 JAGS Models cond.likelihood.model = function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- mu.p53 } C&lt;-1000 for (k in 1:n.discovery){ #zeroes trick for MLE~cond prob tau[k]&lt;- pow(SE[k], -2) L[k]&lt;- dnorm(MLE[k],mu.p53, tau[k])/(pnorm(-q*SE, mu.p53, tau[k]) + 1-pnorm(q*SE, mu.p53, tau[k])) phi[k]&lt;- -log(L[k])+C zeroes[k]~dpois(phi[k]) } mu.p53 ~ dnorm(0,.1) } cond.likelihood.re.model = function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- beta.p53[site[j]] } C&lt;-1000 for (k in 1:n.discovery){ #zeroes trick for MLE~cond prob tau[k]&lt;- pow(SE[k], -2) L[k]&lt;- dnorm(MLE[k],beta.p53[discovery.sites[k]], tau[k])/ (pnorm(-q*SE[k], beta.p53[discovery.sites[k]], tau[k]) + 1-pnorm(q*SE[k], beta.p53[discovery.sites[k]], tau[k])) phi[k]&lt;- -log(L[k])+C zeroes[k]~dpois(phi[k]) } for (i in 1:n.sites){ #total sites beta.p53[i] ~dnorm(mu.p53,phi.p53) } mu.p53 ~ dnorm(0,.1) # phi.p53 ~ dgamma(1, .05) # sigma.p53 &lt;- pow(phi.p53, -.5) phi.p53 &lt;- pow(sigma.p53, -2) sigma.p53 ~ dt(0,1,1)%_%T(0,) } ones.cauchy.model= function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- beta.p53[site[j]] } for (l in 1:n.sites) { beta.p53.1[l] ~ dnorm(mu.p53, phi.p53) beta.p53[l] &lt;- beta.p53.1[l]*(mu.p53.notzero) } #ones trick C&lt;-1e6 epsilon&lt;-0.01 tau&lt;- pow(epsilon,-2) mu.p53.notzero&lt;- step(temp) temp&lt;-dt(mu.p53,0,1, 1)-dnorm(mu.p53,0,tau) #cauchy is same as t with df=1 L&lt;-(dnorm(mu.p53,0,tau)*(1-pind))+(dt(mu.p53,0,1,1)*pind) p &lt;- L/ C one~ dbern(p) mu.p53 ~ dunif(-10,10) # phi.p53 ~ dgamma(1, .05) # sigma.p53 &lt;- pow(phi.p53, -.5) phi.p53 &lt;- pow(sigma.p53, -2) sigma.p53 ~ dt(0,1,1)%_%T(0,) pind ~ dbeta(.5,.5) } ones.normal.model= function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- beta.p53[site[j]] } for (l in 1:n.sites) { beta.p53.1[l] ~ dnorm(mu.p53, phi.p53) beta.p53[l] &lt;- beta.p53.1[l]*(mu.p53.notzero) } #ones trick C&lt;-1e6 epsilon&lt;-0.01 tau&lt;- pow(epsilon,-2) mu.p53.notzero&lt;- step(temp) temp&lt;-dnorm(mu.p53,0,1)-dnorm(mu.p53,0,tau) L&lt;-(dnorm(mu.p53,0,tau)*(1-pind))+(dnorm(mu.p53,0,1)*pind) p &lt;- L/ C one ~ dbern(p) mu.p53 ~ dunif(-10,10) # phi.p53 ~ dgamma(1, .05) # sigma.p53 &lt;- pow(phi.p53, -.5) phi.p53 &lt;- pow(sigma.p53, -2) sigma.p53 ~ dt(0,1,1)%_%T(0,) pind ~ dbeta(.5,.5) } latent.normal.model= function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- beta.p53[site[j]] } for (l in 1:n.sites) { beta.p53.1[l] ~ dnorm(mu.p53, phi.p53) beta.p53[l] &lt;- beta.p53.1[l]*(mu.p53.notzero) } mu.p53&lt;- mu1.p53*mu.p53.notzero mu1.p53 ~ dnorm(0,1) # phi.p53 ~ dgamma(1, .05) # sigma.p53 &lt;- pow(phi.p53, -.5) phi.p53 &lt;- pow(sigma.p53, -2) sigma.p53 ~ dt(0,1,1)%_%T(0,) mu.p53.notzero~dbern(pind) pind ~ dbeta(.5,.5) } latent.cauchy.model= function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- beta.p53[site[j]] } for (l in 1:n.sites) { beta.p53.1[l] ~ dnorm(mu.p53, phi.p53) beta.p53[l] &lt;- beta.p53.1[l]*(mu.p53.notzero) } mu.p53&lt;- mu1.p53*mu.p53.notzero mu1.p53 ~ dt(0,1, 1) # phi.p53 ~ dgamma(1, .05) # sigma.p53 &lt;- pow(phi.p53, -.5) #half cauchy, uniform to 1 or to 5, take a guess on sd phi.p53 &lt;- pow(sigma.p53, -2) sigma.p53 ~ dt(0,1,1)%_%T(0,) mu.p53.notzero~dbern(pind) pind ~ dbeta(.5,.5) } fixed.model = function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- mu.p53 } mu.p53&lt;- mu1.p53*mu.p53.notzero mu1.p53 ~ dnorm(0,.1) mu.p53.notzero~dbern(pind) pind ~ dbeta(.5,.5) } original.model = function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- beta.p53[site[j]] } for (l in 1:n.sites) { beta.p53[l] ~ dnorm(mu.p53, phi.p53) } mu.p53 ~ dnorm(0,1) phi.p53 ~ dgamma(1, .05) sigma.p53 &lt;- pow(phi.p53, -.5) } bf.model = function() { for (j in 1:J) { CaseCon[j] ~ dbern(theta[j]) logit(theta[j]) &lt;- beta.p53[site[j]] } for (l in 1:n.sites) { beta.p53.1[l] ~ dnorm(mu.p53, phi.p53) beta.p53[l] &lt;- beta.p53.1[l]*(mu.p53.notzero) } mu.p53&lt;- mu1.p53*mu.p53.notzero mu1.p53 ~ dt(0,1,1) phi.p53 &lt;- pow(sigma.p53, -2) sigma.p53 ~ dt(0,1,1)%_%T(0,) mu.p53.notzero~dbern(pind) prior.odds&lt;- (1-prior.pind)/prior.pind BF&lt;- 1/(-exp(1)*p*log(p)) #eplogp is BF h0/h1 pind&lt;-prior.odds*BF/(1+prior.odds*BF) prior.pind ~ dbeta(.9,.9) } 7.2 Joint Analysis of TP53 SNPs Table 7.1: Joint Analysis Estimates of \\(\\mu_{p53}\\) estimate 95% CI rs9894946n 0 -0.018 - 0.172 rs12951053n 0 0 - 0 , 0.097 - 0.252 rs1625895n 0 0 - 0 rs1042522n 0 0 - 0 rs2909430n 0 -0.049 - 0.295 rs8079544n 0 0 - 0 rs2287499n 0 0 - 0 , 0.18 - 0.281 rs2287498n 0.27 0.083 - 0.445 rs8073498n 0 0 - 0 rs2078486n 0.372 -0.119 - -0.006 , 0 - 0.8 "],
["references.html", "References", " References "]
]
