[
["index.html", "My Final College Paper Introduction", " My Final College Paper Huijia Yu May 2018 Introduction Welcome to the R Markdown thesis template. This template is based on (and in many places copied directly from) the Reed College LaTeX template, but hopefully it will provide a nicer interface for those that have never used TeX or LaTeX before. Using R Markdown will also allow you to easily keep track of your analyses in R chunks of code, with the resulting plots and output included as well. The hope is this R Markdown template gets you in the habit of doing reproducible research, which benefits you long-term as a researcher, but also will greatly help anyone that is trying to reproduce or build onto your results down the road. Hopefully, you won’t have much of a learning period to go through and you will reap the benefits of a nicely formatted thesis. The use of LaTeX in combination with Markdown is more consistent than the output of a word processor, much less prone to corruption or crashing, and the resulting file is smaller than a Word file. While you may have never had problems using Word in the past, your thesis is likely going to be about twice as large and complex as anything you’ve written before, taxing Word’s capabilities. After working with Markdown and R together for a few weeks, we are confident this will be your reporting style of choice going forward. Why use it? R Markdown creates a simple and straightforward way to interface with the beauty of LaTeX. Packages have been written in R to work directly with LaTeX to produce nicely formatting tables and paragraphs. In addition to creating a user friendly interface to LaTeX, R Markdown also allows you to read in your data, to analyze it and to visualize it using R functions, and also to provide the documentation and commentary on the results of your project. Further, it allows for R results to be passed inline to the commentary of your results. You’ll see more on this later. Having your code and commentary all together in one place has a plethora of benefits! Who should use it? Anyone who needs to use data analysis, math, tables, a lot of figures, complex cross-references, or who just cares about the final appearance of their document should use R Markdown. Of particular use should be anyone in the sciences, but the user-friendly nature of Markdown and its ability to keep track of and easily include figures, automatically generate a table of contents, index, references, table of figures, etc. should make it of great benefit to nearly anyone writing a thesis project. "],
["1-abstract.html", "Chapter 1 Abstract", " Chapter 1 Abstract "],
["2-intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction "],
["3-lit-review.html", "Chapter 3 Literature Review 3.1 Background and Motivation 3.2 Frequentist Methods 3.3 Bayesian Methods", " Chapter 3 Literature Review 3.1 Background and Motivation 3.1.0.1 Redefine statistical significance Benjamin et al 2017 https://www.nature.com/articles/s41562-017-0189-z map Bayes factors to p-values depends on H1 so it needs to be for specific alternatives plots of bayes factors to p values show that 5% threshold is too high suggests .005 to reduce false positives (esp in low power) 3.1.0.2 Calibration of p Values for Testing Precise Null Hypotheses Sellke et al 2003 http://www.tandfonline.com/doi/pdf/10.1198/000313001300339950?needAccess=true transform p value to bayes factor (lower bound on this) and alpha (type 1 error) 3.2 Frequentist Methods 3.2.1 Conditional Likelihood 3.2.1.1 Overcoming the Winner’s Curse: Estimating Penetrance Parameters from Case-Control Data Zollner and Pritchard, 2007 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1852705/ instead of calculating odds-ratio, use prevalance to calculate penetrance likelihood conditional on parameters and significance at level alpha. algorithm: sample from L(\\(\\theta, \\phi\\)) (population freq and penetrance) get L(data | phi, theta) and pick highest perturb each param by epsilon, if better accept to test, generate 3 samples: naive, corrected, and estimated w an independent sample 3.2.1.2 Estimating odds ratios in genome scans: an approximate conditional likelihood approach. Ghosh A, Zou F, Wright FA 2008 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665019/ proposes mean of conditional likelihood, MLE, and mixture of both there used to be code online www.bios.unc.edu/~wrightf/genomebias/ 3.2.1.3 Bias-reduced estimators and confidence intervals for odds ratios in genome-wide association studies Zhong H, Prentice RL. 2008 similar to Zollner and Pritchard, but with asymptotic approximation instead of sampling “slightly different” likelihood formulation 3.2.1.3.1 Statistical correction of the Winner’s Curse explains replication variability in quantitative trait genome-wide association studies Palmer and Pe’er. 2017 based on Zhong and Prentice. code here: https://github.com/cpalmer718/gwas-winners-curse 3.2.2 Bootstrap 3.2.2.1 BR-squared: a practical solution to the winner’s curse in genome-wide scans Sun et al. 2011 https://www.ncbi.nlm.nih.gov/pubmed/21246217 bootstrapping to address ranking effect and threshold effect independent detection and validation/effect estimation using sample split calculate effect for both in and out of bootstrap sample x, then get bias as diff and adjust can use genome wide bootstrap to improve over likelihood based methods software here: http://www.utstat.toronto.edu/sun/Software/BR2/ 3.2.3 Shrinkage 3.2.3.1 Extracting Actionable Information From Genome Scans Bacanu and Kendler. 2012 http://onlinelibrary.wiley.com/doi/10.1002/gepi.21682/full soft threshold, adjusts statistics such that their sum of squares do not overestimate the true mean 3.2.3.2 A simple yet accurate correction for winner’s curse can predict signals discovered in much larger genome scans Bigdelli et al. 2016 https://academic.oup.com/bioinformatics/article/32/17/2598/2450747 uses multiple testing adjustment (False Discovery Rate) FDR Inverse Quantile Transformation (FIQT) transforms to p vals, shrinks towards one, and then transforms back to z scores code: https://github.com/bacanusa/FIQT 3.2.3.2.1 Statistical significance for genomewide studies Storey and Tibshirani. 2003 http://www.pnas.org/content/100/16/9440.full FDR = expectation of false positives/significant features q is “proportion of false positives incurred when calling that feature significant” sort of Bayesian (probability that a feature is null given it is significant) provides estimates for proportion of truly null features empirically there’s several other papers on how to estimate this 3.3 Bayesian Methods 3.3.1 “Fully Bayesian” 3.3.1.1 Bayesian methods to overcome the winner’s curse in genetic studies Xu et al 2011 https://arxiv.org/pdf/0907.2770.pdf Bayesian framework with model averaging logistic regression example with biased estimator, approximated with normal by CLT (hierarchical) Bayesian bias correction spike-and-slab prior for mu, p(mu given epsilon) Beta prior for epsilon p(epsilon given a,b) -unifor prior is problematic because of Bartlett’s paradox inv gamma for sigma sq with mean = s^2 and var = 200 200 chosen bc empirically doesn’t matter much sample from posterior w data augmentation metropolis hasting BMA without conditioning on significance 3.3.2 Empirical Bayes 3.3.2.1 Empirical Bayes Correction for the Winner’s Curse in Genetic Association Studies Ferguson et al 2013 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048064/ prior of mean mu_i from distribution f empirical bayes method find posterior (where f is the prior) and use the mean results in smallest sq error f is unknown, but can express mean and variance of posterior with marginal p(z) can estimate empirical log(p(z)) with binning and then fitting splines nonparametric but still dependent on spline function, bins, etc. can achieve pseudo Bayesian credible intervals these are not appropriate though, because they are symmetric (which is not guaranteed for multimodal or asymmetric distributions) would be better to use HPD combine with cond likelihood to account for tail behavior by comparing length of 95%CI for empirical bayes and conditional likelihood intervals, and pick the shortest simulation study with data from 3 distributions (normal, contamindated normal, double exponential) to test robustness 3.3.2.2 Power estimation and sample size determination for replication studies of genome-wide association studies Jiang and Yu. 2016 https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-2296-4 interested in power “bayesian power” takes the expectation over mu of P(replication significant | original study, mu, H1) spike and slab prior hyperparameters estimated empirically this will underestimate the variance proportion of null hypothesis is based on another parameter gamma corresponding to a p-value threshold can calculate Bayesian predictive power estimator simulation to compare estimate to true power "],
["4-models.html", "Chapter 4 Models", " Chapter 4 Models "],
["5-simulation-study.html", "Chapter 5 Simulation Study", " Chapter 5 Simulation Study "],
["6-application.html", "Chapter 6 Application", " Chapter 6 Application "],
["7-discussion.html", "Chapter 7 Discussion", " Chapter 7 Discussion "],
["references.html", "References", " References "]
]
