# Hierarchical Simulations


```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = FALSE)

library(doParallel)
library(R2jags)
library(random)
library(reshape2)
library(ggplot2)
library(knitr)
require(gridExtra)
source("HPD.R")
```


```{r cond likelihood dat}
get.cond.likelihood.data<- function(data, p = 0.00325){
  freq = glm(CaseCon ~ factor(site), data=data,family=binomial, x=T)
  #get "discovery" with smallest p value (that is significant)
  coefs = coef(summary(freq))
  discovery.site = which.min(coefs[,4])
  if(coefs[discovery.site,4]>p){
    return(NULL)
  }
  
  MLE = coefs[discovery.site,1]
  SE = coefs[discovery.site,2]
  p.value = coefs[discovery.site,4]
  #make new data for model
  exclude <- which(data$site==discovery.site)
  newdata = list(MLE=MLE, SE=SE, n.discovery= 1, zeroes= 0, 
                 discovery.sites=discovery.site,
                 CaseCon= data$CaseCon[-exclude], 
                 site= data$site[-exclude], n.sites = data$n.sites,
                 q=qnorm(1-p/2),
                 p = coefs[discovery.site,4])
  newdata$J<- length(newdata$CaseCon)
  return(newdata)
  
}
```


```{r sampling}

sample<- function(assoc, mu, sd, n.sites=7,observations = 1000){ 
  #assoc is H
  beta.p53 = rnorm(n.sites,mu,sd)*assoc 
  Y <-site <- rep(NA, observations*n.sites)
  for(i in 1:n.sites){
    Y[((i-1)*observations+1):(i*observations)]<-rbinom(observations, 1, exp(beta.p53[i])/(1+exp(beta.p53[i])))
    site[((i-1)*observations+1):(i*observations)]<- rep(i, observations)
  }
  return(list(beta.p53=beta.p53, simdata = list(CaseCon=Y, site=site,  J=n.sites*observations ,n.sites=n.sites, one=1)))
}

```


```{r run fn}
run.all<- function(assoc,mu, sd, inits,n.sites){
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd,n.sites)
    cond.data<-get.cond.likelihood.data(data$simdata)
    count=count+1
  }
  #run jags
  latent.cauchy <- jags(data=data$simdata, inits=inits,
                        parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                        model = latent.cauchy.model)
  cond.likelihood<-NULL
  if(!is.null(cond.data)){
    cond.likelihood<- jags(data=cond.data ,inits=inits,
                           parameters.to.save =c("mu.p53", "sigma.p53"), 
                           model = cond.likelihood.re.model)
    bf.approx<- jags(data=cond.data, inits=inits,
                     parameters.to.save =c("pind","mu.p53", "sigma.p53","mu.p53.notzero"),
                     model = bf.model)
  }
  original<- jags(data=data$simdata, inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"),
                  model = original.model)
  
  return(list(latent.cauchy = latent.cauchy,
              cond.likelihood = cond.likelihood,
              bf=bf.approx,
              original= original,
              beta.p53 = data$beta.p53))
}

```


```{r run cl}
run.cl.better<- function(assoc,mu, sd, inits, pvals){ #slower but will use the same data so hopefully better
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd)
    cond.data<-get.cond.likelihood.data(data$simdata, min(pvals))
    count=count+1
  }
  if(!is.null(cond.data)){
    cond.likelihood<- lapply(pvals, function(p){
      cond.data$q<-qnorm(1-p/2)
      return(jags(data=cond.data ,inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"), 
                  #rerun also with random effect?
                  model = cond.likelihood.re.model))
    })
  }
  return(cond.likelihood)
}


```


```{r parallel, eval= FALSE}
# stopCluster(cl)
cl = makeCluster(8)
registerDoParallel(cl)


mu=0.203;sd= 0.05831085 #from glmer estimates using all data and one snp
assoc<-c(0,1,1,1,1)
muvec<-c(0,mu,mu,mu,mu)
sdvec<-c(1,.5*sd,sd,2*sd,4*sd)

I = 50
source("p53simJAGScode_cauchy.R")
system.time({
  allsim<- foreach(j = 1:5) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 7))
    }
})
save.image()

#this one just has more models (re cl, original)
cl = makeCluster(8)
registerDoParallel(cl)
assoc<-c(1,1,1,1)
muvec<-c(mu,mu,mu,mu)
sdvec<-c(.5*sd,sd,2*sd,4*sd)

system.time({
  allsim.30<- foreach(j = 1:length(muvec)) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 30))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
system.time({
  cl.sim.better.1<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd, NULL,pvals))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
system.time({
  cl.sim.better.4<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd*4, NULL,pvals))
    }
})
save.image()

```

```{r summary stats}
getstatsitems<-c("hasmu","haszero","mean","median",
                 "ismultimodal", "intervallength","probneqzero",
                 "hassd","sdmean","sdmedian","sdintlen",
                 "haspind","pindmean","pindmedian","pindg.5")

modelnames<- c("Bayesian",
               "Cond Likelihood","BF Approx", "Original")

getstatsloop<-function(simlist,mu,sd,modelnames){
  I<-length(simlist)
  vec<-sapply(simlist, function(outputlist) sapply(outputlist[-length(outputlist)],getstats,mu,sd))
  return(array(data=vec,dim= c(length(getstatsitems),length(modelnames),I), dimnames=list(getstatsitems,modelnames,seq(1:I))))
}

getstats<-function(jagsoutput, mu, sd){
  mu.samples<- jagsoutput$BUGSoutput$sims.list$mu.p53
  if(!is.null(jagsoutput$BUGSoutput$sims.list$mu.p53.notzero)){
    mu.samples<-mu.samples*jagsoutput$BUGSoutput$sims.list$mu.p53.notzero
  }
  cred<-HPDM(mu.samples)
  upper<- cred[2]
  lower<- cred[1]
  if(length(cred)>2){
    #assume 2 modes max
    upper <- c(upper,cred[4])
    lower <- c(lower,cred[3])
  }
  sd.samples<-sd.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$sigma.p53)){
    sd.samples <- (jagsoutput$BUGSoutput$sims.list$sigma.p53)
    sd.cred<- HPDM(sd.samples)
  }
  
  pind.samples<-pind.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$pind)){
    pind.samples<- jagsoutput$BUGSoutput$sims.list$pind
    # pind.cred<- HPDM(pind.samples)
  }
  return(c(hasmu = any(upper>=mu&lower<=mu), 
           haszero = any(upper>=0&lower<=0), 
           mean = mean(mu.samples),
           median = median(mu.samples),
           ismultimodal=length(cred)>2,
           intervallength = sum(upper-lower),
           probneqzero = sum(mu.samples!=0)/length(mu.samples),
           hassd= sd.cred[2]>=sd&sd.cred[1]<=sd,
           sdmean= mean(sd.samples),
           sdmedian= median(sd.samples),
           sdintlen=sd.cred[2]-sd.cred[1],
           haspind=pind.cred[2]>=0&pind.cred[1]<=1,
           pindmean =mean(pind.samples),
           pindmedian=median(pind.samples),
           # pindintlen=pind.cred[2]-pind.cred[1]
           pindgreater.5=mean(pind.samples>.5)
           
  ))
}

```


```{r get stats}
sim.0<-getstatsloop(allsim[[1]],0, 1,modelnames)
sim.half<-getstatsloop(allsim[[2]],mu, sd*.5,modelnames)
sim.1<-getstatsloop(allsim[[3]],mu, sd, modelnames)
sim.2<-getstatsloop(allsim[[4]],mu,sd*2,modelnames)
sim.4<-getstatsloop(allsim[[5]],mu,sd*4,modelnames)

```



### Simulations

The data are generated from a hierarchical (i.e. mixed effect) logistic model as discribed in the data section**link**: if truly associated, $\mu$ and $\sigma^2$ have (fixed) nonzero values;$\beta_j \sim N(\mu, \sigma^2)$. Otherwise, $\mu=\beta_j = 0,  \forall j$. The observed data $Y_{ij}$ is binary, and has $P(Y_{ij}=1| \beta_j) = \frac{e^{\beta_j}}{1+e^{\beta_j}}$. The $j$ index corresponds to the "group" to which the observation belongs.

To try to keep this simulation as close to the real data as possible, a preliminary logistic regression with random slope and random p53 coefficient by site was run. This led to the values of $\mu =`r mu` , \sigma^2 = `r sd^2`$. The value of $\mu$ remained fixed through all the simulations, but different values of $\sigma$ were used to test the sensitivity of the models: $\sigma$, $\sigma/2$, $2\sigma$, and  $4\sigma$. The number of sites was set to 7, since results using 30 sites were almost identical. Each site had 1000 observations. A total of `r I` simulated datasets was created each time.


`r I` datasets were also simulated under the null hypothesis. They were fit with the models described above, and resulted in reasonable estimates. 

### Finding "Discovery" Sites 

In this simulation study, a logistic regression with fixed effects for sites was conducted to find the site with the smallest p-value less than $\alpha$. If no sites matched this description, the data was resampled until at least one site was viable. The maximum likelihood estimate of this effect and its variance were added as data for the conditional likelihood model, and the p-value was added to the bayes factor approximation model.  The observations for this site were then taken out of the data.

## Results

### Sensitivity


```{r}
df<- data.frame(melt(t(abs(sim.0[4,,])))[,2:3], simulation=0)
simslist<-list(sim.half,sim.1,sim.2,sim.4)
for (i in 1:4){
  df<-rbind(df,data.frame(melt(t(abs(simslist[[i]][4,,]-mu)))[,2:3], simulation=i))
}
mseplot = ggplot(data = (df), aes(x=Var2, y=value)) +
  geom_boxplot(aes(color=factor(simulation)))+  theme_minimal() +
  labs(x="Model",y="Absolute Error", title=expression(paste("Absolute Error of ",mu)))+
  scale_colour_few(name=expression(paste(sigma^2, " used in simulation")), labels=c(expression(paste(mu, " = 0")),"s/2", "s", "2s", "4s"))+  theme( legend.justification=c(1,1), legend.position=c(1,1))


rmse<- cbind(sapply(1:4, function(x) sqrt(mean((sim.0[4,x,])^2))),
             sapply(simslist, function(sim) 
               sapply(1:4, function(x) sqrt(mean((sim[4,x,]-mu)^2)))))
rownames(rmse)<-modelnames
colnames(rmse)<-c("mu=0","s/2", "s", "2s", "4s")


pindmed<- cbind(sapply(c(1,3), function(x) sqrt(mean((sim.0[14,x,])^2))),
                sapply(simslist, function(sim) 
                  sapply(c(1,3), function(x) sqrt(mean((sim[14,x,])^2)))))
rownames(pindmed)<-modelnames[c(1,3)]
colnames(pindmed)<-c("mu=0","s/2", "s", "2s", "4s")

zeromu<- cbind(sapply(c(1,3), function(x) sqrt(mean((sim.0[7,x,])^2))),
               sapply(simslist, function(sim) 
                 sapply(c(1,3), function(x) sqrt(mean((sim[7,x,])^2)))))
rownames(zeromu)<-modelnames[c(1,3)]
colnames(zeromu)<-c("mu=0","s/2", "s", "2s", "4s")


df<- data.frame(melt(t(abs(sim.0[6,,])))[,2:3], simulation=0)
simslist<-list(sim.half,sim.1,sim.2,sim.4)
for (i in 1:4){
  df<-rbind(df,data.frame(melt(t(abs(simslist[[i]][6,,])))[,2:3], simulation=i))
}

intlenplot = ggplot(data = (df), aes(x=Var2, y=value)) +
  geom_boxplot(aes(color=factor(simulation)))+  theme_minimal() +
  labs(x="Model",y="Credible Interval Length", title=expression(paste("Interval Length of ",mu)))+
  scale_colour_few(name=expression(paste(sigma^2, " used in simulation")), labels=c(expression(paste(mu, " = 0")),"s/2", "s", "2s", "4s"))+  theme( legend.justification=c(1,1), legend.position=c(1,1))

hasboth<- cbind(sapply(1:4, function(x) mean(sim.0[1,x,])),
                sapply(simslist, function(sim) 
                  sapply(1:4, function(x) mean(sim[1,x,]*sim[2,x,]))))
rownames(hasboth)<-modelnames
colnames(hasboth)<-c("mu=0","s/2", "s", "2s", "4s")

hasm<-  sapply(simslist, function(sim) 
  sapply(1:4, function(x) mean(sim[1,x,]*(1-sim[2,x,]))))
rownames(hasm)<-modelnames
colnames(hasm)<-c("s/2", "s", "2s", "4s")

has0notmu<-  sapply(simslist, function(sim) 
  sapply(1:4, function(x) mean(sim[2,x,]*(1-sim[1,x,]))))
rownames(has0notmu)<-modelnames
colnames(has0notmu)<-c("s/2", "s", "2s", "4s")

```

#### Mu

##### RMSE

As expected, the models performed more poorly as $\sigma$ increased. Out of the three proposed models (compared with the original), the fully bayesian and BF approximation models performed best when there was no true effect (since they were the only ones that that had this option). At small variances (s/2, s,  2s), the original and bayesian models outperform the others, but at 4s the conditional likelihood model has the lowest RMSE. 

```{r}
mseplot
kable(rmse, caption=expression(paste("RMSE of ",mu)))
```
##### Coverage

The BF approximation model is the most conservative, with the intervals covering 0 more times than any other model for all values of $\sigma$. All models have very high coverage in general.

The Bayesian model had the shortest intervals. 

```{r}
kable(hasm)
kable(has0notmu)
kable(hasboth)
kable(hasboth[,2:5]+hasm)
intlenplot
```



#### Xi

While one would expect the probability of being associated ($\xi$) to also increase with $\sigma$, this was not true for either the fully bayesian model nor the bayes factor approximation one, both of which had consistent posterior estimates of $\xi$. Similarly, the proportion of nonzero $\mu$ samples from the posterior (this is the same as the proportion of times the latent variable $i = 1$), was almost 1 for the truly associated cases, and close to zero for true null. One thing to consider is that under the null hypothesis, the variance across sites would actually be zero, which is why the models identified the association so decisively. 

```{r}
pindmed
zeromu
```




### Sensitivity of Conditional Likelihood Method to Changes in $\alpha$


```{r cl analysis}
#can use the other fn when rerunning so
#sim.cl.stats.better<-getstatsloop(cl.sim.better,0, 1, pvals) #13 modes?

sim.cl.stats.better1<-getstatsloop(cl.sim.better.1,mu,sd, pvals) 
cl.better.plots1<-make_plots(make_simlist(sim.cl.stats.better1), mu,sd,pvals)

sim.cl.stats.better4<-getstatsloop(cl.sim.better.4,mu,sd, pvals) 
cl.better.plots4<-make_plots(make_simlist(sim.cl.stats.better4), mu,sd,pvals)
```


The conditional likelihood method is robust to changes in the level $\alpha$. To test this, we consider 5 different levels: $0.05, 0.01, 0.005, 0.001, 10^{-7}$. `r I` datasets were sampled, for which at least one location was significant at the smallest $\alpha$ level. The conditional likelihood model with random effects and without was then fitted for each level. 



```{r}
#cl plots
df<- rbind(data.frame(melt(t(abs(sim.cl.stats.better1[4,,])))[,2:3], simulation=1),
           data.frame(melt(t(abs(sim.cl.stats.better4[4,,])))[,2:3], simulation=2))

mseplot = ggplot(data = (df), aes(x=factor(Var2), y=value)) +
  geom_boxplot(aes(color=factor(simulation)))+  theme_minimal() + 
  labs(x="Model",y="Absolute Error", title=expression(paste("Absolute Error of ",mu)))+
  scale_colour_few(name=expression(paste(sigma^2, " used in simulation")), labels=c( "s", "4s"))+  theme( legend.justification=c(1,1), legend.position=c(1,1))

rmse<- cbind(sapply(1:5, function(x) sqrt(mean((sim.cl.stats.better1[4,x,]-mu)^2))),
               sapply(1:5, function(x) sqrt(mean((sim.cl.stats.better4[4,x,]-mu)^2))))
rownames(rmse)<-pvals
colnames(rmse)<-c("s", "4s")



df<- rbind(data.frame(melt(t(abs(sim.cl.stats.better1[6,,])))[,2:3], simulation=1),
           data.frame(melt(t(abs(sim.cl.stats.better4[6,,])))[,2:3], simulation=2))

intlenplot = ggplot(data = (df), aes(x=factor(Var2), y=value)) +
  geom_boxplot(aes(color=factor(simulation)))+  theme_minimal() + 
  labs(x="Model",y="Credible Interval Length", title=expression(paste("Credible Interval of ",mu)))+
  scale_colour_few(name=expression(paste(sigma^2, " used in simulation")), labels=c( "s", "4s"))+  theme( legend.justification=c(1,1), legend.position=c(1,1))
kable(rmse)
mseplot
intlenplot
```

This model shows little difference across levels because the conditional likelihood prior is being applied to the specific site, not to the global mean, which has the same distribution as it would if the likelihood of $\beta$ was unconditional.

A simpler model with no random effects ($MLE_i \sim N(\mu, SE_i)$) and small variance resulted in larger credible intervals for smaller $\alpha$, even though the point estimates were consistent. This may seem counterintuitive, since we would expect that the lower $\alpha$ would yield more "precise" results, but makes sense based on the shape of the prior on $\mu$. There is no difference between credible intervals of this same model when the simulations have higher variance. This is because the single significant site is overpowered by the rest of them, which are highly likely to not be significant because they are too spread out.

