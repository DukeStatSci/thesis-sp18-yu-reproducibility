# Hierarchical Simulations

```{r}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(cache = FALSE)
mu = 2
sd = 1
I = 100
```


```{r}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(cache = FALSE)

library(doParallel)
library(R2jags)
library(random)
library(reshape2)
library(ggplot2)
library(knitr)
require(gridExtra)
source("HPD.R")
```


```{r cond likelihood dat}
get.cond.likelihood.data<- function(data, p = 0.00325){
  freq = glm(CaseCon ~ factor(site), data=data,family=binomial, x=T)
  #get "discovery" with smallest p value (that is significant)
  coefs = coef(summary(freq))
  discovery.site = which.min(coefs[,4])
  if(coefs[discovery.site,4]>p){
    return(NULL)
  }
  
  MLE = coefs[discovery.site,1]
  SE = coefs[discovery.site,2]
  p.value = coefs[discovery.site,4]
  #make new data for model
  exclude <- which(data$site==discovery.site)
  newdata = list(MLE=MLE, SE=SE, n.discovery= 1, zeroes= 0, 
                 discovery.sites=discovery.site,
                 CaseCon= data$CaseCon[-exclude], 
                 site= data$site[-exclude], n.sites = data$n.sites,
                 q=qnorm(1-p/2),
                 p = coefs[discovery.site,4])
  newdata$J<- length(newdata$CaseCon)
  return(newdata)
  
}
```


```{r sampling}

sample<- function(assoc, mu, sd, n.sites=7,observations = 1000){ 
  #assoc is H
  beta.p53 = rnorm(n.sites,mu,sd)*assoc 
  Y <-site <- rep(NA, observations*n.sites)
  for(i in 1:n.sites){
    Y[((i-1)*observations+1):(i*observations)]<-rbinom(observations, 1, exp(beta.p53[i])/(1+exp(beta.p53[i])))
    site[((i-1)*observations+1):(i*observations)]<- rep(i, observations)
  }
  return(list(beta.p53=beta.p53, simdata = list(CaseCon=Y, site=site,  J=n.sites*observations ,n.sites=n.sites, one=1)))
}

```


```{r run fn}
run.all<- function(assoc,mu, sd, inits,n.sites){
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd,n.sites)
    cond.data<-get.cond.likelihood.data(data$simdata)
    count=count+1
  }
  #run jags
  latent.cauchy <- jags(data=data$simdata, inits=inits,
                        parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                        model = latent.cauchy.model)
  cond.likelihood<-NULL
  if(!is.null(cond.data)){
    cond.likelihood<- jags(data=cond.data ,inits=inits,
                              parameters.to.save =c("mu.p53", "sigma.p53"), 
                              model = cond.likelihood.re.model)
    bf.approx<- jags(data=cond.data, inits=inits,
                  parameters.to.save =c("pind","mu.p53", "sigma.p53","mu.p53.notzero"),
                  model = bf.model)
  }
  original<- jags(data=data$simdata, inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"),
                  model = original.model)
  
  return(list(latent.cauchy = latent.cauchy,
              cond.likelihood = cond.likelihood,
              bf=bf.approx,
              original= original,
              beta.p53 = data$beta.p53))
}

```


```{r run cl}
run.cl.better<- function(assoc,mu, sd, inits, pvals){ #slower but will use the same data so hopefully better
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd)
    cond.data<-get.cond.likelihood.data(data$simdata, min(pvals))
    count=count+1
  }
  if(!is.null(cond.data)){
    cond.likelihood<- lapply(pvals, function(p){
      cond.data$q<-qnorm(1-p/2)
      return(jags(data=cond.data ,inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"), 
                  #rerun also with random effect?
                  model = cond.likelihood.re.model))
    })
  }
  return(cond.likelihood)
}


```


```{r parallel}
# stopCluster(cl)
cl = makeCluster(8)
registerDoParallel(cl)


mu=0.203;sd= 0.05831085 #from glmer estimates using all data and one snp
assoc<-c(0,1,1,1,1)
muvec<-c(0,mu,mu,mu,mu)
sdvec<-c(1,.5*sd,sd,2*sd,4*sd)

I = 50
source("p53simJAGScode_cauchy.R")
system.time({
  allsim<- foreach(j = 1:5) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 7))
    }
})
save.image()

#this one just has more models (re cl, original)
cl = makeCluster(8)
registerDoParallel(cl)
assoc<-c(1,1,1,1)
muvec<-c(mu,mu,mu,mu)
sdvec<-c(.5*sd,sd,2*sd,4*sd)

system.time({
  allsim.30<- foreach(j = 1:length(muvec)) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 30))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
system.time({
  cl.sim.better.1<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd, NULL,pvals))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
system.time({
  cl.sim.better.4<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd*4, NULL,pvals))
    }
})
save.image()

```

```{r summary stats}
getstatsitems<-c("hasmu","haszero","mean","median",
                 "ismultimodal", "intervallength","probneqzero",
                 "hassd","sdmean","sdmedian","sdintlen",
                 "haspind","pindmean","pindmedian","pindg.5")

modelnames<- c("Bayesian",
               "Cond Likelihood","BF Approx", "Original")

getstatsloop<-function(simlist,mu,sd,modelnames){
  I<-length(simlist)
  vec<-sapply(simlist, function(outputlist) sapply(outputlist[-length(outputlist)],getstats,mu,sd))
  return(array(data=vec,dim= c(length(getstatsitems),length(modelnames),I), dimnames=list(getstatsitems,modelnames,seq(1:I))))
}

getstats<-function(jagsoutput, mu, sd){
  mu.samples<- jagsoutput$BUGSoutput$sims.list$mu.p53
  if(!is.null(jagsoutput$BUGSoutput$sims.list$mu.p53.notzero)){
    mu.samples<-mu.samples*jagsoutput$BUGSoutput$sims.list$mu.p53.notzero
  }
  cred<-HPDM(mu.samples)
  upper<- cred[2]
  lower<- cred[1]
  if(length(cred)>2){
    #assume 2 modes max
    upper <- c(upper,cred[4])
    lower <- c(lower,cred[3])
  }
  sd.samples<-sd.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$sigma.p53)){
  sd.samples <- (jagsoutput$BUGSoutput$sims.list$sigma.p53)
  sd.cred<- HPDM(sd.samples)
  }
  
  pind.samples<-pind.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$pind)){
    pind.samples<- jagsoutput$BUGSoutput$sims.list$pind
    # pind.cred<- HPDM(pind.samples)
  }
  return(c(hasmu = any(upper>=mu&lower<=mu), 
           haszero = any(upper>=0&lower<=0), 
           mean = mean(mu.samples),
           median = median(mu.samples),
           ismultimodal=length(cred)>2,
           intervallength = sum(upper-lower),
           probneqzero = sum(mu.samples!=0)/length(mu.samples),
           hassd= sd.cred[2]>=sd&sd.cred[1]<=sd,
           sdmean= mean(sd.samples),
           sdmedian= median(sd.samples),
           sdintlen=sd.cred[2]-sd.cred[1],
           haspind=pind.cred[2]>=0&pind.cred[1]<=1,
           pindmean =mean(pind.samples),
           pindmedian=median(pind.samples),
           # pindintlen=pind.cred[2]-pind.cred[1]
           pindgreater.5=mean(pind.samples>.5)
           
  ))
}

```


```{r get stats}
sim.0<-getstatsloop(allsim[[1]],0, 1,modelnames)
sim.half<-getstatsloop(allsim[[2]],mu, sd*.5,modelnames)
sim.1<-getstatsloop(allsim[[3]],mu, sd, modelnames)
sim.2<-getstatsloop(allsim[[4]],mu,sd*2,modelnames)
sim.4<-getstatsloop(allsim[[5]],mu,sd*4,modelnames)

```





```{r plotting fn}
make_simlist<-function(sim){
  return(lapply(seq(dim(sim)[2]), function(x) t(sim[ , x, ])))
}

make_estimate_error_plot<-function(simlist, means,medians, true,modelnames,plottitle = "Absolute Error"){
  mean.sqerr<- data.frame(sapply(simlist, function(x) abs(x[,means]-true)))
  colnames(mean.sqerr)<-modelnames
  
  median.sqerr<- data.frame(sapply(simlist, function(x) abs(x[,medians]-true)))
  colnames(median.sqerr)<-modelnames
  df <- data.frame(apply(mean.sqerr,2,function(x) sqrt(mean(x^2))),
                   apply(median.sqerr,2,function(x) sqrt(mean(x^2))))
  colnames(df)<-c("Posterior Mean", "Posterior Median")
  mean.sqerr$estimator <- rep("mean",dim(mean.sqerr)[1])
  median.sqerr$estimator <- rep("median",dim(median.sqerr)[1])
  
  sqerr<- rbind(mean.sqerr,median.sqerr)
  return(list(plot = ggplot(data = melt(sqerr), 
                            aes(x=variable, y=value,color=estimator)) +
                geom_boxplot()+labs(x= "Model", y="Absolute Value",
                                    title= plottitle),
              table = df))
}

make_plots<-function(simlist, mu, sd, modelnames){
  #mse
  muerr<- (make_estimate_error_plot(simlist,3,4,mu,modelnames,"Mu Error Distribution"))
  
  #bar plots for coverage, times it's right
  coverage<- data.frame(sapply(simlist, function(x) x[,1]))
  colnames(coverage)<-modelnames
  
  containszero<- data.frame(sapply(simlist, function(x) x[,2]))
  both<- coverage*containszero
  coverage[coverage==1]<-2
  coverage[containszero==1]<-1
  coverage[both==1]<-3
  coverage1<-melt(coverage)
  levelnames<-c("Neither", "Only Zero","Only Mu","Both")
  coverage1$value<-factor(levelnames[coverage1$value+1])
  mucoverage= table(coverage1)/dim(simlist[[1]])[1]

  # disjoint<- data.frame(sapply(simlist, function(x) x[,5]))
  # d0 = (containszero==0)*(disjoint)
  # colnames(d0) = modelnames
  # print(kable(table(melt(d0))))
  #interval length
  intlen<- data.frame(sapply(simlist, function(x) x[,6]))
  colnames(intlen)<-modelnames
  
  plot3<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "Intervals"))
  #prob of zero
  p0<- data.frame(sapply(simlist, function(x) x[,7]))
  colnames(p0)<-modelnames
  
  plot4 <- (ggplot(data = melt(p0), aes(x=variable, y=value)) +
              geom_boxplot()+labs(x= "Model", y="proportion of mu!=0", 
                                  title= "Posterior proportion of nonzero mu"))
  
  ################sd, pind
  sderr<- (make_estimate_error_plot(simlist, 9,10,sd,modelnames,"Absolute Error of Standard Deviation"))
  
  pinderr<- (make_estimate_error_plot(simlist, 13,14,0,modelnames, "Probability of H1 (xi) "))
  
  intlen<- data.frame(sapply(simlist, function(x) x[,11]))
  colnames(intlen)<-modelnames
  plot7<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "SD Intervals"))
  sdcov<- data.frame(sapply(simlist, function(x) x[,8]))
  colnames(sdcov)<-modelnames
  sdcov1<-melt(sdcov)
  sdcov1$value<-ifelse(sdcov1$value==1,  "Contains","Does Not Contain")
  sdcoverage<-table(melt(sdcov1))/dim(simlist[[1]])[1]
  
  return(list(plots = list(muerr$plot, plot3,plot4,sderr$plot,
                           pinderr$plot,plot7),
              tables = list(muerr$table, sderr$table, pinderr$table,
                            mucoverage, sdcoverage)))
}

```



```{r}
plots0<-make_plots(make_simlist(sim.0), 0,0, modelnames)
plots1<-make_plots(make_simlist(sim.1), mu,sd,modelnames)
plots2<-make_plots(make_simlist(sim.2), mu,2*sd,modelnames)
plotshalf<-make_plots(make_simlist(sim.half), mu,sd/2,modelnames)
plots4<-make_plots(make_simlist(sim.4), mu,4*sd,modelnames)

```


```{r cl analysis}
#can use the other fn when rerunning so
#sim.cl.stats.better<-getstatsloop(cl.sim.better,0, 1, pvals) #13 modes?

sim.cl.stats.better1<-getstatsloop(cl.sim.better.1,mu,sd, pvals) 
cl.better.plots1<-make_plots(make_simlist(sim.cl.stats.better1), mu,sd,pvals)

sim.cl.stats.better4<-getstatsloop(cl.sim.better.4,mu,sd, pvals) 
cl.better.plots4<-make_plots(make_simlist(sim.cl.stats.better4), mu,sd,pvals)
```

### Simulations

The data are generated from a hierarchical (i.e. mixed effect) logistic model as discribed in the data section**link**: if truly associated, $\mu$ and $\sigma^2$ have (fixed) nonzero values;$\beta_j \sim N(\mu, \sigma^2)$. Otherwise, $\mu=\beta_j = 0,  \forall j$. The observed data $Y_{ij}$ is binary, and has $P(Y_{ij}=1| \beta_j) = \frac{e^{\beta_j}}{1+e^{\beta_j}}$. The $j$ index corresponds to the "group" to which the observation belongs.

To try to keep this simulation as close to the real data as possible, a preliminary logistic regression with random slope and random p53 coefficient by site was run. This led to the values of $\mu =`r mu` , \sigma^2 = `r sd^2`$. The value of $\mu$ remained fixed through all the simulations, but different values of $\sigma$ were used to test the sensitivity of the models: $\sigma$, $\sigma/2$, $2\sigma$, and  $4\sigma$. The number of sites was set to 7, since results using 30 sites were almost identical. Each site had 1000 observations.

A total of `r I` simulations were run.

### Finding "Discovery" Sites 

In this simulation study, a logistic regression with fixed effects for sites was conducted to find the site with the smallest p-value less than $\alpha$. If no sites matched this description, the data was resampled until at least one site was viable. The maximum likelihood estimate of this effect and its variance were added as data for the conditional likelihood model, and the p-value was added to the bayes factor approximation model.  The observations for this site were then taken out of the data.

##### Some Analysis

### General things


### Sensitivity of conditional likelihood method to changes in $\alpha$

The conditional likelihood method is not robust to changes in the level $\alpha$. To test this, we consider 5 different levels: $0.05, 0.01, 0.005, 0.001, 10^{-7}$. 100 datasets were sampled, for which at least one location was significant at the smallest $\alpha$ level. The conditional likelihood model for each level was then fitted. The results from these show that, while the posterior mean estimate was the same for different $\alpha$, the credible intervals got larger as $\alpha$ decreased. This may seem counterintuitive, since we would expect that the lower $\alpha$ would yield more "precise" results, but this can be explained by the fact that the models take into account the selection mechanism. This means that, if an estimate has p-value much lower than $\alpha$, the model will be more confident that it came from a nonzero distribution than if it is close to $\alpha$.

One thing to note is that this only holds for distributions of $\beta$ that have small enough variance that there are no (or very few) sites are less than or equal to zero. Running the exact same simulation with the data coming from a more spread out distribution for $\mu$, we find that there is no difference between models. This is because the single significant site is overpowered by the rest of them, which are highly likely to not be significant because they are too spread out.

```{r}
lapply(cl.better.plots1$tables, kable)
lapply(clplots1$tables, kable)


```


### Sensitivity

To test the robustness of the models (and their mixing ability), the simulations were run with a range of standard deviations:  $\frac{\sigma}{2},\sigma, 2\sigma,4\sigma$. The rest of the parameters remained the same.
As expected, the models performed more poorly as $\sigma$ increased. However, while one would expect the probability of being associated ($\xi$) to also increase with $\sigma$, this was not true for any of the implementations, which had consistent posterior estimates of $\xi$. This means that this model is robust, since it does not need a highly concentrated distribution to detect that there is a true effect. Another thing to consider is that under the null hypothesis (i.e. that there is no association), the variance across sites would actually be close to zero, since they would all have an effect of 0.


```{r}
lapply(plotshalf$tables[c(1,3,4)],kable) #these don't have the sigma^2 samples
lapply(plots1$tables[c(1,3,4)],kable)
lapply(plots2$tables[c(1,3,4)],kable)
lapply(plots4$tables[c(1,3,4)],kable)

lapply(plotshalf$tables[3],kable) #these don't have the sigma^2 samples
lapply(plots1$tables[3],kable)
lapply(plots2$tables[3],kable)
lapply(plots4$tables[3],kable)
```




### Probability of H0 v H1

`r I` datasets were also simulated under the null hypothesis. They were fit with the models described above, and resulted in reasonable estimates. In this case, the proportion of samples with $\mu = 0$ was quite high for all models, and the point estimate of $\xi$ is also less than $0.5$. The conditional effect model has a larger bias than the other models, but this is still very small relative to the scale of $\mu$. The cauchy prior latent variable model had the smallest intervals. Surprisingly, the cauchy ones' trick model had wider intervals than the normal model with the ones' trick (and than both latent variable models), but still had lower coverage. 

```{r}
plots4
```




### some thoughts on bayes factor p value approx

What this really does is skew the beta prior towards one. **see plot**. This model does well in detecting when there is really no effect because the data is able to pull the value of $\xi$ towards zero. However, if the data does have an effect and the distribution is already skewed, the overall effect is almost the same as having the original model (i.e. assuming the alternative).