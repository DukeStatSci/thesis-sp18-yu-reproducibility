# Hierarchical Simulations


```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)

library(doParallel)
library(R2jags)
library(random)
library(reshape2)
library(ggplot2)
library(ggthemes)
library(knitr)
require(gridExtra)
source("HPD.R")
source("p53simJAGScode_cauchy.R")

load("p53sim_processeddata.RData")
mu=0.203;sd= 0.05831085 #from glmer estimates using all data and one snp
assoc<-c(0,1,1,1,1)
muvec<-c(0,mu,mu,mu,mu)
sdvec<-c(1,.5*sd,sd,2*sd,4*sd)
I = 100
pvals=c(.05,.01,.005,.001, 1e-7)

```


```{r cond likelihood dat}
get.cond.likelihood.data<- function(data, p = 0.00325){
  freq = glm(CaseCon ~ factor(site), data=data,family=binomial, x=T)
  #get "discovery" with smallest p value (that is significant)
  coefs = coef(summary(freq))
  discovery.site = which.min(coefs[,4])
  if(coefs[discovery.site,4]>p){
    return(NULL)
  }
  
  MLE = coefs[discovery.site,1]
  SE = coefs[discovery.site,2]
  p.value = coefs[discovery.site,4]
  #make new data for model
  exclude <- which(data$site==discovery.site)
  newdata = list(MLE=MLE, SE=SE, n.discovery= 1, zeroes= 0, 
                 discovery.sites=discovery.site,
                 CaseCon= data$CaseCon[-exclude], 
                 site= data$site[-exclude], n.sites = data$n.sites,
                 q=qnorm(1-p/2),
                 p = coefs[discovery.site,4])
  newdata$J<- length(newdata$CaseCon)
  return(newdata)
  
}
```


```{r sampling}

sample<- function(assoc, mu, sd, n.sites=7,observations = 1000){ 
  #assoc is H
  beta.p53 = rnorm(n.sites,mu,sd)*assoc 
  Y <-site <- rep(NA, observations*n.sites)
  for(i in 1:n.sites){
    Y[((i-1)*observations+1):(i*observations)]<-rbinom(observations, 1, exp(beta.p53[i])/(1+exp(beta.p53[i])))
    site[((i-1)*observations+1):(i*observations)]<- rep(i, observations)
  }
  return(list(beta.p53=beta.p53, simdata = list(CaseCon=Y, site=site,  J=n.sites*observations ,n.sites=n.sites, one=1)))
}

```


```{r run fn}
run.all<- function(assoc,mu, sd, inits,n.sites){
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd,n.sites)
    cond.data<-get.cond.likelihood.data(data$simdata)
    count=count+1
  }
  #run jags
  latent.cauchy <- jags(data=data$simdata, inits=inits,
                        parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                        model = latent.cauchy.model)
  cond.likelihood<-NULL
  if(!is.null(cond.data)){
    cond.likelihood<- jags(data=cond.data ,inits=inits,
                           parameters.to.save =c("mu.p53", "sigma.p53"), 
                           model = cond.likelihood.re.model)
    bf.approx<- jags(data=cond.data, inits=inits,
                     parameters.to.save =c("pind","mu.p53", "sigma.p53","mu.p53.notzero"),
                     model = bf.model)
  }
  original<- jags(data=data$simdata, inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"),
                  model = original.model)
  
  return(list(latent.cauchy = latent.cauchy,
              cond.likelihood = cond.likelihood,
              bf=bf.approx,
              original= original,
              beta.p53 = data$beta.p53))
}

```


```{r run cl}
run.cl.better<- function(assoc,mu, sd, inits, pvals){ #slower but will use the same data so hopefully better
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd)
    cond.data<-get.cond.likelihood.data(data$simdata, min(pvals))
    count=count+1
  }
  if(!is.null(cond.data)){
    cond.likelihood<- lapply(pvals, function(p){
      cond.data$q<-qnorm(1-p/2)
      return(jags(data=cond.data ,inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"), 
                  #rerun also with random effect?
                  model = cond.likelihood.re.model))
    })
  }
  return(cond.likelihood)
}


```


```{r parallel, eval= FALSE}
# stopCluster(cl)
cl = makeCluster(8)
registerDoParallel(cl)

system.time({
  allsim<- foreach(j = 1:5) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 7))
    }
})
save.image()

#this one just has more models (re cl, original)
cl = makeCluster(8)
registerDoParallel(cl)
assoc<-c(1,1,1,1)
muvec<-c(mu,mu,mu,mu)
sdvec<-c(.5*sd,sd,2*sd,4*sd)

system.time({
  allsim.30<- foreach(j = 1:length(muvec)) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 30))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
system.time({
  cl.sim.better.1<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd, NULL,pvals))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
system.time({
  cl.sim.better.4<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd*4, NULL,pvals))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
system.time({
  cl.sim.better.0<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(0,0,sd, NULL,pvals[1:4]))
    }
})
save.image()

```

```{r summary stats}
getstatsitems<-c("hasmu","haszero","mean","median",
                 "ismultimodal", "intervallength","probneqzero",
                 "hassd","sdmean","sdmedian","sdintlen",
                 "haspind","pindmean","pindmedian","pindg.5")

modelnames<- c("Bayesian",
               "Cond Likelihood","BF Approx", "Original")

getstatsloop<-function(simlist,mu,sd,modelnames){
  I<-length(simlist)
  vec<-sapply(simlist, function(outputlist) sapply(outputlist[-length(outputlist)],getstats,mu,sd))
  return(array(data=vec,dim= c(length(getstatsitems),length(modelnames),I), dimnames=list(getstatsitems,modelnames,seq(1:I))))
}

getstats<-function(jagsoutput, mu, sd){
  mu.samples<- jagsoutput$BUGSoutput$sims.list$mu.p53
  if(!is.null(jagsoutput$BUGSoutput$sims.list$mu.p53.notzero)){
    mu.samples<-mu.samples*jagsoutput$BUGSoutput$sims.list$mu.p53.notzero
  }
  cred<-HPDM(mu.samples)
  upper<- cred[2]
  lower<- cred[1]
  if(length(cred)>2){
    #assume 2 modes max
    upper <- c(upper,cred[4])
    lower <- c(lower,cred[3])
  }
  sd.samples<-sd.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$sigma.p53)){
    sd.samples <- (jagsoutput$BUGSoutput$sims.list$sigma.p53)
    sd.cred<- HPDM(sd.samples)
  }
  
  pind.samples<-pind.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$pind)){
    pind.samples<- jagsoutput$BUGSoutput$sims.list$pind
    # pind.cred<- HPDM(pind.samples)
  }
  return(c(hasmu = any(upper>=mu&lower<=mu), 
           haszero = any(upper>=0&lower<=0), 
           mean = mean(mu.samples),
           median = median(mu.samples),
           ismultimodal=length(cred)>2,
           intervallength = sum(upper-lower),
           probneqzero = sum(mu.samples!=0)/length(mu.samples),
           hassd= sd.cred[2]>=sd&sd.cred[1]<=sd,
           sdmean= mean(sd.samples),
           sdmedian= median(sd.samples),
           sdintlen=sd.cred[2]-sd.cred[1],
           haspind=pind.cred[2]>=0&pind.cred[1]<=1,
           pindmean =mean(pind.samples),
           pindmedian=median(pind.samples),
           # pindintlen=pind.cred[2]-pind.cred[1]
           pindgreater.5=mean(pind.samples>.5)
           
  ))
}

```


```{r get stats, eval= FALSE}
sim.0<-getstatsloop(allsim[[1]],0, 1,modelnames)
sim.half<-getstatsloop(allsim[[2]],mu, sd*.5,modelnames)
sim.1<-getstatsloop(allsim[[3]],mu, sd, modelnames)
sim.2<-getstatsloop(allsim[[4]],mu,sd*2,modelnames)
sim.4<-getstatsloop(allsim[[5]],mu,sd*4,modelnames)

sim.cl.stats.better1<-getstatsloop(cl.sim.better.1,mu,sd, pvals) 
sim.cl.stats.better4<-getstatsloop(cl.sim.better.4,mu,sd, pvals) 
sim.cl.stats.better0<-getstatsloop(cl.sim.better.0 ,0,1, pvals) 

save(sim.0, sim.half,sim.1,sim.2,sim.4, sim.cl.stats.better1,sim.cl.stats.better4,sim.cl.stats.better0, file = "p53sim_processeddata.RData")

```


## Data Generation Procedure

The data are generated from a hierarchical (i.e. mixed effect) logistic model as discribed in the data section: if truly associated, $\mu$ and $\sigma^2$ have (fixed) nonzero values;$\beta_j \sim \textsf{N}(\mu, \sigma^2)$. Otherwise, $\mu=\beta_j = 0,  \forall j$. The observed data $Y_{ij}$ is binary, and has $P(Y_{ij}=1| \beta_j) = \frac{e^{\beta_j}}{1+e^{\beta_j}}$. The $j$ index corresponds to the "group" to which the observation belongs.

$$P(Y_{ij}=1) = \textsf{logit}^{-1}(\beta_{j})\\
\beta_{j}|H_1 \sim \textsf{N}(\mu, \sigma^{2})\\
\beta_{j}|H_0 = 0$$

To try to keep this simulation as close to the real data as possible, a preliminary logistic regression with random slope and random p53 coefficient by site was run. This led to the values of $\mu =`r mu` , \sigma^2 = `r round(sd^2, 3)`$. The value of $\mu$ remained fixed through all the simulations, but different values of $\sigma$ were used to test the sensitivity of the models: $\sigma$, $\sigma/2$, $2\sigma$, and  $4\sigma$. The number of sites was set to 7, since results using 30 sites were almost identical. Each site had 1000 observations. A total of `r I` simulated datasets was created each time.


`r I` datasets were also simulated under the null hypothesis. They were fit with the models described above, and resulted in reasonable estimates. 

### Finding "Discovery" Sites 

In this simulation study, a logistic regression with fixed effects for sites was conducted to find the site with the smallest p-value less than $\alpha$. If no sites matched this description, the data was resampled until at least one site was viable. The maximum likelihood estimate of this effect and its variance were added as data for the conditional likelihood model, and the p-value was added to the bayes factor approximation model.  The observations for this site were then taken out of the data.

## Results

### Sensitivity


```{r}
df1<- data.frame(melt(t(abs(sim.0[4,,])))[,2:3], simulation=0)
simslist<-list(sim.half,sim.1,sim.2,sim.4)
for (i in 1:4){
  df1<-rbind(df1,data.frame(melt(t(abs(simslist[[i]][4,,]-mu)))[,2:3], simulation=i))
}
mseplot = ggplot(data = (df1), aes(x=factor(simulation, labels=c(0,"s/2", "s", "2s", "4s")), y=value)) +
  geom_boxplot(aes(color=Var2))+  theme_minimal() +
  labs(x=expression(paste(sigma^2, " used in simulation")),y="Absolute Error", title=expression(paste("Absolute Error of Simulation ",mu )))+
  scale_colour_few(name="Model") +  theme( legend.justification=c(0,1), legend.position=c(0,1))


rmse<- cbind(sapply(1:4, function(x) sqrt(mean((sim.0[4,x,])^2))),
             sapply(simslist, function(sim) 
               sapply(1:4, function(x) sqrt(mean((sim[4,x,]-mu)^2)))))
rownames(rmse)<-modelnames
colnames(rmse)<-c("mu=0","s/2", "s", "2s", "4s")


pindmed<- cbind(sapply(c(1,3), function(x) sqrt(mean((sim.0[14,x,])^2))),
                sapply(simslist, function(sim) 
                  sapply(c(1,3), function(x) sqrt(mean((sim[14,x,])^2)))))
rownames(pindmed)<-modelnames[c(1,3)]
colnames(pindmed)<-c("mu=0","s/2", "s", "2s", "4s")

zeromu<- cbind(sapply(c(1,3), function(x) sqrt(mean((sim.0[7,x,])^2))),
               sapply(simslist, function(sim) 
                 sapply(c(1,3), function(x) sqrt(mean((sim[7,x,])^2)))))
rownames(zeromu)<-modelnames[c(1,3)]
colnames(zeromu)<-c("mu=0","s/2", "s", "2s", "4s")


df<- data.frame(melt(t(abs(sim.0[6,,])))[,2:3], simulation=0)
simslist<-list(sim.half,sim.1,sim.2,sim.4)
for (i in 1:4){
  df<-rbind(df,data.frame(melt(t(abs(simslist[[i]][6,,])))[,2:3], simulation=i))
}

intlenplot = ggplot(data = (df), aes(x=factor(simulation, labels=c(0,"s/2", "s", "2s", "4s")), y=value)) +
  geom_boxplot(aes(color=Var2))+  theme_minimal() +
  labs(x=expression(paste(sigma^2, " used in simulation")),y="Credible Interval Length", title=expression(paste("Credible Intervals of Simulation ",mu )))+
  scale_colour_few(name="Model") +  theme( legend.justification=c(0,1), legend.position=c(0,1))




hasm<-  cbind(sapply(1:4, function(x) mean(sim.0[1,x,])),
              sapply(simslist, function(sim) 
  sapply(1:4, function(x) mean(sim[1,x,]))))
rownames(hasm)<-modelnames
colnames(hasm)<-c("mu=0","s/2", "s", "2s", "4s")

has0<-  cbind(sapply(1:4, function(x) mean(sim.0[1,x,])),
              sapply(simslist, function(sim) 
  sapply(1:4, function(x) mean(sim[2,x,]))))
rownames(has0)<-modelnames
colnames(has0)<-c("mu=0","s/2", "s", "2s", "4s")
```

#### $\mu$

##### RMSE

As expected, the models performed more poorly as $\sigma$ increased. Out of the three proposed models (compared with the original), the fully bayesian and BF approximation models performed best when there was no true effect (since they were the only ones that that had this option). However, there were some simulated datasets where the bayes factor model estimate was actually nonzero and quite large, which suggests that it is not nearly as reliable as the bayesian model.


At small variances (s/2, s,  2s), the original and bayesian models outperform the others. This is not surprising since the other models only have access to $\frac{6}{7}$ of the data. The Bayesian model actually has a slightly higher lower RMSE than the other models when there is a true association.



```{r}
mseplot
kable(rmse, caption="RMSE of $\\mu$")
```

##### Coverage

The conditional likelihood and BF approximation models are the most conservative, with the intervals covering 0 more times than the  others for large values of $\sigma$. All models have very high coverage in general.

The Bayesian model had the shortest intervals, and the original model had the largest. Thus, even though the coverage and RMSE are around the same, the new models are preferable to the original. This does not apply to the simulation with $\sigma=4s$, because $4s>\mu$, which leads to more negative site effects. Thus, it makes sense for models to have wider credible intervals for these simulations. 

```{r}
kable(hasm, ,format='latex',caption="Proportion of Credible Intervals containing $\\mu$")
```

```{r}
kable(has0, ,format='latex',caption="Proportion of Credible Intervals containing 0")
```

```{r}
intlenplot
```


#### $\xi$

While one would expect the probability of being associated ($\xi$) to also increase with $\sigma$, this was not true for either the fully bayesian model nor the bayes factor approximation one, both of which had consistent posterior estimates of $\xi$. Similarly, the proportion of nonzero $\mu$ samples from the posterior (this is the same as the proportion of times the latent variable $i = 1$), was almost 1 for the truly associated cases, and close to zero for true null. One thing to consider is that under the null hypothesis, the variance across sites would actually be zero, which is why the models identified the association so decisively.

The Bayes Factor approximation model has much larger median $\xi$ and $\iota$ because the mass of the distribution of $\xi$ is shifted towards 1. Thus, even though the median of $\iota$ is quite low (and rhe median of $\mu$ is 0), the median of $\xi$ is greater than $0.5$.

```{r}
kable(pindmed,caption="Average of Posterior Median $\\xi$")
```

```{r}
kable(zeromu,caption="Average of Mean of $\\iota$")
```


### Sensitivity of Conditional Likelihood Method to Changes in $\alpha$

### With Random Effects

The conditional likelihood method with random effects is robust to changes in the level $\alpha$. To test this, we consider 5 different levels: $0.05, 0.01, 0.005, 0.001, 10^{-7}$. `r I` datasets were sampled, for which at least one location was significant at the smallest $\alpha$ level. The conditional likelihood model with random effects and without was then fitted for each level. 



```{r}
#cl plots
df<- rbind(data.frame(melt(t(abs(sim.cl.stats.better0[4,,])))[,2:3], simulation=0),
           data.frame(melt(t(abs(sim.cl.stats.better1[4,,]-mu)))[,2:3], simulation=1),
           data.frame(melt(t(abs(sim.cl.stats.better4[4,,]-mu)))[,2:3], simulation=2))

mseplot = ggplot(data = (df), aes(x=factor(simulation, labels=c( "0", "s", "4s")), y=value)) +
  geom_boxplot(aes(color=factor(Var2)))+  theme_minimal() +
  labs(x=expression(paste(sigma^2, " used in simulation")),y="Absolute Error", title=expression(paste("Absolute Error of Simulation ",mu )))+
  scale_colour_few(name="p-value") +  theme( legend.justification=c(0,1), legend.position=c(0,1))


rmse<- cbind(c(sapply(1:4, function(x) sqrt(mean((sim.cl.stats.better0[4,x,])^2))), NA),
             sapply(1:5, function(x) sqrt(mean((sim.cl.stats.better1[4,x,]-mu)^2))),
               sapply(1:5, function(x) sqrt(mean((sim.cl.stats.better4[4,x,]-mu)^2))))
rownames(rmse)<-pvals

colnames(rmse)<-c("0","s", "4s")



df<- rbind(data.frame(melt(t(abs(sim.cl.stats.better0[6,,])))[,2:3], simulation=0),
           data.frame(melt(t(abs(sim.cl.stats.better1[6,,])))[,2:3], simulation=1),
           data.frame(melt(t(abs(sim.cl.stats.better4[6,,])))[,2:3], simulation=2))

intlenplot = ggplot(data = (df), aes(x=factor(simulation, labels=c("0","s","4s")), y=value)) +
  geom_boxplot(aes(color=factor(Var2)))+  theme_minimal() +
  labs(x=expression(paste(sigma^2, " used in simulation")),y="Credible Interval Length", title=expression(paste("Credible Intervals of Simulation ",mu )))+
  scale_colour_few(name="p-value") +  theme( legend.justification=c(0,1), legend.position=c(0,1))

kable(rmse,caption="RMSE of $\\mu$")
mseplot
intlenplot
```

This model shows little difference across levels.


#### No Random Effects

A simpler model than the one proposed has no random effects: $\beta_j\sim \textsf{N}(\mu, \sigma^2)$ and $MLE_i \sim \textsf{N}(\mu, SE_i)$. 



```{r}
#cl plots
load("p53sim_processeddata_extra.RData")
df<- rbind(data.frame(melt(t(abs(sim.cl.stats.global.better0[4,,])))[,2:3], simulation=0),
           data.frame(melt(t(abs(sim.cl.stats.global.better1[4,,]-mu)))[,2:3], simulation=1),
           data.frame(melt(t(abs(sim.cl.stats.global.better4[4,,]-mu)))[,2:3], simulation=2))

mseplot = ggplot(data = (df), aes(x=factor(simulation, labels=c( "0", "s", "4s")), y=value)) +
  geom_boxplot(aes(color=factor(Var2)))+  theme_minimal() +
  labs(x=expression(paste(sigma^2, " used in simulation")),y="Absolute Error", title=expression(paste("Absolute Error of Simulation ",mu )))+
  scale_colour_few(name="p-value") +  theme( legend.justification=c(0,1), legend.position=c(0,1))


rmse<- cbind(c(sapply(1:4, function(x) sqrt(mean((sim.cl.stats.global.better0[4,x,])^2))), NA),
             sapply(1:5, function(x) sqrt(mean((sim.cl.stats.global.better1[4,x,]-mu)^2))),
               sapply(1:5, function(x) sqrt(mean((sim.cl.stats.global.better4[4,x,]-mu)^2))))
rownames(rmse)<-pvals

colnames(rmse)<-c("0","s", "4s")



df<- rbind(data.frame(melt(t(abs(sim.cl.stats.global.better0[6,,])))[,2:3], simulation=0),
           data.frame(melt(t(abs(sim.cl.stats.global.better1[6,,])))[,2:3], simulation=1),
           data.frame(melt(t(abs(sim.cl.stats.global.better4[6,,])))[,2:3], simulation=2))

intlenplot = ggplot(data = (df), aes(x=factor(simulation, labels=c("0","s","4s")), y=value)) +
  geom_boxplot(aes(color=factor(Var2)))+  theme_minimal() +
  labs(x=expression(paste(sigma^2, " used in simulation")),y="Credible Interval Length", title=expression(paste("Credible Intervals of Simulation ",mu )))+
  scale_colour_few(name="p-value") +  theme( legend.justification=c(0,1), legend.position=c(0,1))

kable(rmse,caption="RMSE of $\\mu$")
mseplot
intlenplot
```

Removing the uncertainty for site-specific means leads to a reduction in the length of the credible intervals for $\mu$. The interval length in the random effect model was larger for simulations with large variance between sites ($\sigma^2 = 4s$) than for simulations with small variance ($\sigma^2 = s$), which appropriately captures the uncertainty of the estimate for $\mu$. However, the model with no random effects creates credible intervals that are the same length for simulations with $\sigma^2 = 4s$ and $\sigma^2 = s$. The RMSE is very similar for the two models for each $\sigma^2$, and neither model's credible interval length or median estimate changes with $\alpha$. 
