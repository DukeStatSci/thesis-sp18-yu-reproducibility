```{r}
library(doParallel)
library(R2jags)
library(random)
library(reshape2)
library(ggplot2)
library(knitr)
require(gridExtra)
source("HPD.R")
```


```{r cond likelihood}
get.cond.likelihood.data<- function(data, p = 0.00325){
  freq = glm(CaseCon ~ factor(site), data=data,family=binomial, x=T)
  #get "discovery" with smallest p value (that is significant)
  coefs = coef(summary(freq))
  discovery.site = which.min(coefs[,4])
  if(coefs[discovery.site,4]>p){
    return(NULL)
  }
  
  MLE = coefs[discovery.site,1]
  SE = coefs[discovery.site,2]
  #make new data for model
  exclude <- which(data$site==discovery.site)
  newdata = list(MLE=MLE, SE=SE, n.discovery= 1, zeroes= 0, 
                 discovery.sites=discovery.site,
                 CaseCon= data$CaseCon[-exclude], 
                 site= data$site[-exclude], n.sites = data$n.sites,
                 q=qnorm(1-p/2))
  newdata$J<- length(newdata$CaseCon)
  return(newdata)
  
}
```


```{r sampling}

sample<- function(assoc, mu, sd, n.sites=7,observations = 1000){ 
  #assoc is H
  beta.p53 = rnorm(n.sites,mu,sd)*assoc 
  Y <-site <- rep(NA, observations*n.sites)
  for(i in 1:n.sites){
    Y[((i-1)*observations+1):(i*observations)]<-rbinom(observations, 1, exp(beta.p53[i])/(1+exp(beta.p53[i])))
    site[((i-1)*observations+1):(i*observations)]<- rep(i, observations)
  }
  return(list(beta.p53=beta.p53, simdata = list(CaseCon=Y, site=site,  J=n.sites*observations ,n.sites=n.sites, one=1)))
}

```


```{r run fn}
run.all<- function(assoc,mu, sd, inits,n.sites){
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd,n.sites)
    cond.data<-get.cond.likelihood.data(data$simdata)
    count=count+1
  }
  #run jags
  ones.cauchy <- jags(data=data$simdata, inits=inits, 
                      parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                      model = ones.cauchy.model)
  ones.normal <- jags(data=data$simdata, inits=inits,
                      parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                      model = ones.normal.model)
  latent.normal <- jags(data=data$simdata, inits=inits,
                        parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                        model = latent.normal.model)
  latent.cauchy <- jags(data=data$simdata, inits=inits,
                        parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                        model = latent.cauchy.model)
  cond.likelihood<-NULL
  if(!is.null(cond.data)){
    cond.likelihood <- jags(data=cond.data ,inits=inits,
                            parameters.to.save =c("mu.p53"), 
                            model = cond.likelihood.model)
    cond.likelihood.re<- jags(data=cond.data ,inits=inits,
                              parameters.to.save =c("mu.p53", "sigma.p53"), 
                              model = cond.likelihood.re.model)
  }
  
  fixed <- jags(data=data$simdata, inits=inits, 
                parameters.to.save =c("pind", "mu.p53","mu.p53.notzero"), 
                model = fixed.model)
  original<- jags(data=data$simdata, inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"),
                  model = original.model)
  
  return(list(ones.cauchy = ones.cauchy, ones.normal=ones.normal,
              latent.normal = latent.normal,latent.cauchy = latent.cauchy,
              cond.likelihood = cond.likelihood,fixed = fixed, 
              original= original,cl.re=cond.likelihood.re, beta.p53 = data$beta.p53))
}

```


```{r parallel}
# stopCluster(cl)
cl = makeCluster(8)
registerDoParallel(cl)


mu=0.203;sd= 0.05831085 #from glmer estimates using all data
assoc<-c(0,1,1,1,1)
muvec<-c(0,mu,mu,mu,mu)
sdvec<-c(1,.5*sd,sd,2*sd,4*sd)

I = 10
source("p53simJAGScode.R")
system.time({
  allsim<- foreach(j = 1:5) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 7))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
source("p53simJAGScode_uniform.R")
system.time({
  allsim.u<- foreach(j = 1:5) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 7))
    }
})

cl = makeCluster(8)
registerDoParallel(cl)
save.image()
source("p53simJAGScode_cauchy.R")
system.time({
  allsim.c<- foreach(j = 1:5) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 7))
    }
})
save.image()

```

```{r}
#this one just has more models (re cl, original)
cl = makeCluster(8)
registerDoParallel(cl)
assoc<-c(1,1)
muvec<-c(mu,mu)
sdvec<-c(sd,4*sd)

I = 10 #getDoParWorkers()*10
#I=1

system.time({
  allsim.30<- foreach(j = 1:2) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 30))
    }
})
save.image()
```


```{r run cl}
run.cl.better<- function(assoc,mu, sd, inits, pvals){ #slower but will use the same data so hopefully better
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd)
    cond.data<-get.cond.likelihood.data(data$simdata, min(pvals))
    count=count+1
  }
  if(!is.null(cond.data)){
    cond.likelihood<- lapply(pvals, function(p){
      cond.data<-get.cond.likelihood.data(data$simdata, p)
      return(jags(data=cond.data ,inits=inits,
                  parameters.to.save =c("mu.p53", "phi.p53"), 
                  #rerun also with random effect?
                  model = cond.likelihood.model))
    })
  }
  return(list(cond.likelihood, data$beta.p53))
}

##delete this once things are done
run.cl<- function(assoc,mu, sd, inits, p){
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd)
    cond.data<-get.cond.likelihood.data(data$simdata, p)
    count=count+1
  }
  if(!is.null(cond.data)){
    cond.likelihood <- jags(data=cond.data ,inits=inits,
                            parameters.to.save =c("pind", "mu.p53", "sigma.p53"), 
                            model = cond.likelihood.model)
  }
  return(cond.likelihood)
}

```

```{r parallel cl}
#remove these two once we can run bettercl with regular sd
cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
I=100
system.time({
  cl.sim<- foreach(j = 1:length(pvals)) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl(1,mu,sd, NULL,pvals[j]))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
I=100
system.time({
  cl.sim2<- foreach(j = 1:length(pvals)) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl(1,mu,sd*4, NULL,pvals[j]))
    }
})
save.image()
####

cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
I=100
system.time({
  cl.sim.better<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd*4, NULL,pvals))
    }
})
save.image()

```

```{r summary stats}
getstatsitems<-c("hasmu","haszero","mean","median",
                 "ismultimodal", "intervallength","probzero",
                 "hassd","sdmean","sdmedian","sdintlen",
                 "haspind","pindmean","pindmedian","pindintlen")

modelnames<- c("Cauchy-Ones Trick","Normal-Ones Trick",
               "Normal-Latent","Cauchy-Latent",
               "Cond Likelihood", "Fixed Effect", "Original","CL RE")

getstatsloop<-function(simlist,mu,sd,modelnames){
  I<-length(simlist)
  vec<-sapply(simlist, function(outputlist) sapply(outputlist[-length(outputlist)],getstats,mu,sd))
  return(array(data=vec,dim= c(length(getstatsitems),length(modelnames),I), dimnames=list(getstatsitems,modelnames,seq(1:I))))
}

getstats<-function(jagsoutput, mu, sd){
  mu.samples<- jagsoutput$BUGSoutput$sims.list$mu.p53
  if(!is.null(jagsoutput$BUGSoutput$sims.list$mu.p53.notzero)){
    mu.samples<-mu.samples*jagsoutput$BUGSoutput$sims.list$mu.p53.notzero
  }
  cred<-HPDM(mu.samples)
  upper<- cred[2]
  lower<- cred[1]
  if(length(cred)>2){
    #assume 2 modes max
    upper <- c(upper,cred[4])
    lower <- c(lower,cred[3])
  }
  sd.samples<-sd.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$sigma.p53)){
  sd.samples <- (jagsoutput$BUGSoutput$sims.list$sigma.p53)
  sd.cred<- HPDM(sd.samples)
  }
  
  pind.samples<-pind.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$pind)){
    pind.samples<- jagsoutput$BUGSoutput$sims.list$pind
    pind.cred<- HPDM(pind.samples)
  }
  return(c(hasmu = any(upper>=mu&lower<=mu), 
           haszero = any(upper>=0&lower<=0), 
           mean = mean(mu.samples),
           median = median(mu.samples),
           ismultimodal=length(cred)>2,
           intervallength = sum(upper-lower),
           probzero = sum(mu.samples==0)/length(mu.samples),
           hassd= sd.cred[2]>=sd&sd.cred[1]<=sd,
           sdmean= mean(sd.samples),
           sdmedian= median(sd.samples),
           sdintlen=sd.cred[2]-sd.cred[1],
           haspind=pind.cred[2]>=0&pind.cred[1]<=1,
           pindmean =mean(pind.samples),
           pindmedian=median(pind.samples),
           pindintlen=pind.cred[2]-pind.cred[1]
           
  ))
}

```


```{r get stats}
sim.0<-getstatsloop(allsim[[1]],0, 1,modelnames[1:6]) #13 modes?
sim.half<-getstatsloop(allsim[[2]],mu, sd*.5,modelnames[1:6])
sim.1<-getstatsloop(allsim[[3]],mu, sd, modelnames[1:6])
sim.2<-getstatsloop(allsim[[4]],mu,sd*2,modelnames[1:6])
sim.4<-getstatsloop(allsim.5[[1]],mu,sd*4,modelnames[1:6])

```





```{r plotting fn}
make_simlist<-function(sim){
  return(lapply(seq(dim(sim)[2]), function(x) t(sim[ , x, ])))
}

make_estimate_error_plot<-function(simlist, means,medians, true,modelnames,plottitle = "Absolute Error"){
  mean.sqerr<- data.frame(sapply(simlist, function(x) abs(x[,means]-true)))
  colnames(mean.sqerr)<-modelnames
  
  median.sqerr<- data.frame(sapply(simlist, function(x) abs(x[,medians]-true)))
  colnames(median.sqerr)<-modelnames
  df <- data.frame(apply(mean.sqerr,2,function(x) sqrt(mean(x^2))),
                   apply(median.sqerr,2,function(x) sqrt(mean(x^2))))
  colnames(df)<-c("Posterior Mean", "Posterior Median")
  mean.sqerr$estimator <- rep("mean",dim(mean.sqerr)[1])
  median.sqerr$estimator <- rep("median",dim(median.sqerr)[1])
  
  sqerr<- rbind(mean.sqerr,median.sqerr)
  return(list(plot = ggplot(data = melt(sqerr), 
                            aes(x=variable, y=value,color=estimator)) +
                geom_boxplot()+labs(x= "Posterior Mean", y="Absolute Error",
                                    title= plottitle),
              table = df))
}

make_plots<-function(simlist, mu, sd, modelnames){
  #mse
  muerr<- (make_estimate_error_plot(simlist,3,4,mu,modelnames,"Mu Error Distribution"))
  
  #bar plots for coverage, times it's right
  coverage<- data.frame(sapply(simlist, function(x) x[,1]))
  colnames(coverage)<-modelnames
  
  containszero<- data.frame(sapply(simlist, function(x) x[,2]))
  both<- coverage*containszero
  coverage[coverage==1]<-2
  coverage[containszero==1]<-1
  coverage[both==1]<-3
  coverage1<-melt(coverage)
  levelnames<-c("Neither", "Only Zero","Only Mu","Both")
  coverage1$value<-factor(levelnames[coverage1$value+1])
  mucoverage= table(coverage1)/length(simlist[[1]])

  # disjoint<- data.frame(sapply(simlist, function(x) x[,5]))
  # d0 = (containszero==0)*(disjoint)
  # colnames(d0) = modelnames
  # print(kable(table(melt(d0))))
  #interval length
  intlen<- data.frame(sapply(simlist, function(x) x[,6]))
  colnames(intlen)<-modelnames
  
  plot3<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "Intervals"))
  #prob of zero
  p0<- data.frame(sapply(simlist, function(x) x[,7]))
  colnames(p0)<-modelnames
  
  plot4 <- (ggplot(data = melt(p0), aes(x=variable, y=value)) +
              geom_boxplot()+labs(x= "Model", y="probability of mu=0", 
                                  title= "Posterior Probability of Null (H=0)"))
  
  ################sd, pind
  sderr<- (make_estimate_error_plot(simlist, 9,10,sd,modelnames,"Absolute Error of Standard Deviation"))
  
  pinderr<- (make_estimate_error_plot(simlist, 13,14,0,modelnames, "Absolute Error of Probability of H0"))
  
  intlen<- data.frame(sapply(simlist, function(x) x[,11]))
  colnames(intlen)<-modelnames
  plot7<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "SD Intervals"))
  intlen<- data.frame(sapply(simlist, function(x) x[,15]))
  colnames(intlen)<-modelnames
  plot8<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "pind Intervals"))
  sdcov<- data.frame(sapply(simlist, function(x) x[,8]))
  colnames(sdcov)<-modelnames
  sdcov1<-melt(sdcov)
  sdcov1$value<-ifelse(sdcov1$value==1,  "Contains","Does Not Contain")
  sdcoverage<-table(melt(sdcov1))/length(simlist[[1]])
  
  return(list(plots = list(muerr$plot, plot3,plot4,sderr$plot,
                           pinderr$plot,plot7,plot8),
              tables = list(muerr$table, sderr$table, pinderr$table,
                            mucoverage, sdcoverage)))
}

```



```{r}
plots0<-make_plots(make_simlist(sim.0), 0,0, modelnames[1:6])
plots1<-make_plots(make_simlist(sim.1), mu,sd,modelnames[1:6])
plots2<-make_plots(make_simlist(sim.2), mu,2*sd,modelnames[1:6])
plotshalf<-make_plots(make_simlist(sim.2), mu,sd/2,modelnames[1:6])
plots4<-make_plots(make_simlist(sim.4), mu,4*sd,modelnames[1:6])

```


```{r cl analysis}
clplots1<-make_plots(lapply(cl.sim, function(x) t(sapply(x, getstats,mu,sd,1))), mu, sd,1,pvals)

clplots2<-make_plots(lapply(cl.sim2, function(x) t(sapply(x, getstats,mu,sd,1))), mu, sd,1,pvals)

#can use the other fn when rerunning so
#sim.cl.stats.better<-getstatsloop(cl.sim.better,0, 1, pvals) #13 modes?

vec <- sapply(cl.sim.better, function(x) sapply(x, getstats,mu,sd,1))
sim.cl.stats.better <-array(data=vec,dim= c(length(getstatsitems),length(pvals),I))

cl.better.plots<-make_plots(make_simlist(sim.cl.stats.better), mu,sd,pvals)
```

#### assorted notes/explanations/stuff- will move around once things are more set

### Simulations: 

The data are generated from a hierarchical (i.e. mixed effect) logistic model as follows: if truly associated, $\mu$ and $\sigma^2$ have (fixed) nonzero values;$\beta_j \sim N(\mu, \sigma^2)$. Otherwise, $\mu=\beta_j = 0,  \forall j$. The observed data $Y_{ij}$ is binary, and has $P(Y_{ij}=1| \beta_j) = \frac{e^{\beta_j}}{1+e^{\beta_j}}$. The $j$ index corresponds to the "group" to which the observation belongs.

To try to keep this simulation as close to the real data as possible, a preliminary logistic regression with random slope and random p53 coefficient by site was run. This led to the values of $\mu =`r mu` , \sigma^2 = `r sd^2`$ (see [Frequentist Results]). The value of $\mu$ remained fixed through all the simulations, but different values of $\sigma$ were used to test the sensitivity of the models: $\sigma$, $\sigma/2$, $2\sigma$, and  $4\sigma$. The number of sites was set to 7 and 30 to compare the effect of sites. Each site had 1000 observations.

A total of `r I` simulations were run.

### Models 

1. The probability of association $\xi$ can give rise to a latent variable drawn from a bernoulli, which is then used to parametrize the distribution of $\mu$. That is,  $\pi(\mu|\iota) = (1-\iota ) N(\mu, 0, \epsilon)+ \iota N(\mu,0 ,1)$, and $P(\iota = 1 ) = \xi$. This is the latent variable model with normal prior. Then $\xi \sim Beta(1/2, 1/2), \sigma \sim invGamma(1, .05), \beta_j \sim N(\mu, \sigma^2), Y_{ij} \sim logit^{-1}(\beta_j)$.

2. The point mass can be approximated using a very concentrated normal distribution, centered at 0. Thus, $\pi(\mu|\xi) = (1-\xi) N(\mu, 0, \epsilon)+ \xi N(\mu,0 ,1)$. The other priors are the same as in the latent variable model.

Models that do not use the latent variable must specify the mixture distribution in JAGS directly. This is done through the ones trick, which use the Bernoulli distribution in order to allow for arbitrary distributions. Consider a prior  for $\theta$ that is proportional to $\pi(\theta)$. If we set that bernoulli variable "ones" is equal to 1 with probability $\pi(\theta)$, create an observation "ones"$= 1$, and set a uniform prior for $\theta$, then we are effectively creating a "posterior" for theta that is proportional to  $\pi(\theta)$ as intended.

In these scenarios, the model from which a sample comes from is also not evident. To determine whether a small value is truly from the concentrated normal, we can compute the probability of the sample for the two distributions and choose the model that results in a larger one.

3. and 4. The "ones-cauchy" and "latent-cauchy" have the same structure as the original ones, but use a $Cauchy(0,1)$ prior for $\mu$ to allow for larger effects without increasing the variance and making the prior too diffuse, which could lead to Lindley's paradox. In the case of the ones model, the point mass approximation is kept as a Normal.

5. The "fixed" model is just the same as the latent variable model with normal prior, but without the site effect.

6. The "original" model is a slightly modified version of the one used in the previous analysis of the TP53 data (see Data section **link or st here**). This has a normal prior for $\mu$, the global effect. This model is equivalent to the mixture model given the alternative (i.e. that mu is significant or $\xi = 1$).

7. and 8. The conditional likelihood model takes into account the probability of a "significant" event occurring when calculating its likelihood (i.e. the likelihood conditional on having significant data). In this model, we differentiate between the discovery and validation data. Given the discovery sites' MLE and SE, we can use the CLT and definition of MLE to state that $MLE_i \sim N(\mu, SE_i)$. Conditioning on the fact that this estimate is significant, $P(MLE_i) = \frac{\phi(MLE_i, \mu, SE_i)}{\Phi(-q, \mu, SE_i)+1-\Phi(q, \mu, SE_i)}$, where $\phi(x, \mu, \sigma)$ is the pdf of a normal distribution with mean $\mu$ and variance $\sigma^2$, and $\Phi(x, \mu, \sigma)$ is the cdf of the same distribution. The value of $q$ is $\Phi^{-1}(1-\frac{\alpha}{2}, 0 ,SE_i)$, and $\alpha$ is the power of the test (that is, p-values that are greater than $\alpha$ are not considered significant). Let this distribution be denoted as $CL(\mu,SE_i, q)$. The MLE and SE are sufficient statistics.

In this simulation study, a logistic regression with fixed effects for sites was conducted to find the site with the smallest p-value less than $\alpha$. If no sites matched this description, the data was resampled until at least one site was viable. The observations for this site were then taken out of the data, and the maximum likelihood estimate of this effect and its variance were added for the Bayesian model. The full specification of this model is as follows:
$Y_{ij} \sim logit^{-1}(\mu)$,
$MLE_k \sim CL(\mu,SE_k, q)$,
$\mu \sim N(0, .1)$.
The validation sites are modeled with the mixed effect model with a normal prior for $\mu$ and inverse gamma for $\sigma$.

There is one model specified with random effects and one without. The fixed effect model is described above, and the random effect model assumes $MLE_i$ comes from a normal distribution centered at site effect $\beta_i$ rather than at $\mu$, and that the validation data $Y_{ij} \sim N(\beta_j, \sigma)$ as in the fully Bayesian models.


Posterior inference for all models was based on running 2,000 iterations of a Markov chain Monte Carlo (MCMC) algorithm using JAGS**version etc**.

##### Some Analysis

### General things

We compute Bayesian 95% credible intervals by using the highest posterior density (HPD), taking the 95% of the sampled values with the highest density. The coverage is based on whether or not the true value is within this interval.

Point estimates for $\mu, \sigma^2, \xi$ were calculated used posterior mean and median; both are shown below.


### Sensitivity of conditional likelihood method to changes in $\alpha$

The conditional likelihood method is not robust to changes in the level $\alpha$. To test this, we consider 5 different levels: $0.05, 0.01, 0.005, 0.001, 10^{-7}$. 100 datasets were sampled, for which at least one location was significant at the smallest $\alpha$ level. The conditional likelihood model for each level was then fitted. The results from these show that, while the posterior mean estimate was the same for different $\alpha$, the credible intervals got larger as $\alpha$ decreased. This may seem counterintuitive, since we would expect that the lower $\alpha$ would yield more "precise" results, but this can be explained by the fact that the models take into account the selection mechanism. This means that, if an estimate has p-value much lower than $\alpha$, the model will be more confident that it came from a nonzero distribution than if it is close to $\alpha$.

One thing to note is that this only holds for distributions of $\beta$ that have small enough variance that there are no (or very few) sites are less than or equal to zero. Running the exact same simulation with the data coming from a more spread out distribution for $\mu$, we find that there is no difference between models. This is because the single significant site is overpowered by the rest of them, which are highly likely to not be significant because they are too spread out.

```{r}
lapply(cl.better.plots$tables, kable)
lapply(clplots1$tables, kable)


```


### Sensitivity

To test the robustness of the models (and their mixing ability), the simulations were run with a range of standard deviations:  $\frac{\sigma}{2},\sigma, 2\sigma,4\sigma$. The rest of the parameters remained the same.
As expected, the models performed more poorly as $\sigma$ increased. However, while one would expect the probability of being associated ($\xi$) to also increase with $\sigma$, this was not true for the implementations using the ones trick, indicating that **a**.

```{r}
plots2

```




### Probability of H0 v H1

Simulated datasets that were not truly associated were also fit with the models described above, all of which did well, as expected. 

```{r}
plots0
```



### Number of Sites

Although using 7 sites more closely mimics the real data, these are not enough to get a good estimate if we consider each site as an observation of sorts in the hierarchical model. Thus, to test the behaviour of the priors for $\sigma$, this number was increased to 30, which led to significantly improved estimates for sigma, showing that although the inverse gamma was not the best fit, part of the reason it led to overestimates of theta was the small number of sites.

```{r}
plots.30.1<-make_plots(make_simlist(sim.30.1), mu,sd,modelnames[1:6])

plots1$tables
plots.30.1$tables
```



### mu and zero part
JAGS of latent v ones trick- potentially have some traceplots to show
```{r}
#simulation 68 in sim1
traceplot(allsim[[3]][[68]][[1]], varname="mu.p53") #ones, normal
traceplot(allsim[[3]][[68]][[2]], varname="mu.p53") #ones, cauchy
traceplot(allsim[[3]][[68]][[3]], varname="mu.p53") #latent, normal
traceplot(allsim[[3]][[68]][[4]], varname="mu.p53") #latent, cauchy

allsim[[3]][[68]][[7]] #true beta
```
 

One problem that arises when using the ones trick is that the markov chain does not sample from the entire space, but rather gets stuck within a subset of values. This leads to posteriors for $\mu$ that are either positive or negative, with no zero values. In the example shown, all true values of $\beta_j$ are greater than zero; however, the chains that were initialized with negative values in the latent variable models never reach positive or zero values for $\mu$.


### sigma

Both the uniform and the truncated cauchy lead to better estimates of the variance than the inverse gamma on simulated datasets with 7 sites and small true variance. The cauchy prior estimates have smaller RMSE than the uniform prior for the mixed models using latent variables, the original normal model, and the random effect (although the difference is small). The uniform produces better posterior mean estimates for the ones trick specifications. However, regardless of the prior used, the latent variable models are still closer to the true values.

```{r}
kable(plots1$tables[[5]], caption = "Coverage of Variance")
kable(plots1.u$tables[[5]], caption = "Coverage of Variance using Uniform Prior")
kable(plots1.c$tables[[5]], caption = "Coverage of Variance using Truncated Cauchy Prior")



```




