```{r}
library(doParallel)
library(R2jags)
library(random)
library(reshape2)
library(ggplot2)
library(knitr)
require(gridExtra)
source("HPD.R")
source("p53simJAGScode.R")
```


```{r cond likelihood}
get.cond.likelihood.data<- function(data, p = 0.00325){
  freq = glm(CaseCon ~ factor(site), data=data,family=binomial, x=T)
  #get "discovery" with smallest p value (that is significant)
  coefs = coef(summary(freq))
  discovery.site = which.min(coefs[,4])
  if(coefs[discovery.site,4]>p){
    return(NULL)
  }
  
  MLE = coefs[discovery.site,1]
  SE = coefs[discovery.site,2]
  #make new data for model
  exclude <- which(data$site==discovery.site)
  newdata = list(MLE=MLE, SE=SE, n.discovery= 1, zeroes= 0, 
                 discovery.sites=discovery.site,
                 CaseCon= data$CaseCon[-exclude], 
                 site= data$site[-exclude], n.sites = data$n.sites,
                 q=qnorm(1-p/2))
  newdata$J<- length(newdata$CaseCon)
  return(newdata)
  
}
```


```{r sampling}
#true mu fixed, smaller variance to make sure it is above 0 (especially since it's learned anyway)

sample<- function(assoc, mu, sd, n.sites=7,observations = 1000){ #assoc is H
  beta.p53 = rnorm(n.sites,mu,sd)*assoc 
  Y <-site <- rep(NA, observations*n.sites)
  for(i in 1:n.sites){
    Y[((i-1)*observations+1):(i*observations)]<-rbinom(observations, 1, exp(beta.p53[i])/(1+exp(beta.p53[i])))
    site[((i-1)*observations+1):(i*observations)]<- rep(i, observations)
  }
  return(list(beta.p53=beta.p53, simdata = list(CaseCon=Y, site=site,  J=n.sites*observations ,n.sites=n.sites, one=1)))
}

```


```{r run fn}
run.all<- function(assoc,mu, sd, inits,n.sites){
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd,n.sites)
    cond.data<-get.cond.likelihood.data(data$simdata)
    count=count+1
  }
  #run jags
  ones.cauchy <- jags(data=data$simdata, inits=inits, 
                      parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                      model = ones.cauchy.model)
  ones.normal <- jags(data=data$simdata, inits=inits,
                      parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                      model = ones.normal.model)
  latent.normal <- jags(data=data$simdata, inits=inits,
                        parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                        model = latent.normal.model)
  latent.cauchy <- jags(data=data$simdata, inits=inits,
                        parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"),
                        model = latent.cauchy.model)
  cond.likelihood<-NULL
  if(!is.null(cond.data)){
    cond.likelihood <- jags(data=cond.data ,inits=inits,
                            parameters.to.save =c("mu.p53", "sigma.p53"), 
                            model = cond.likelihood.model)
    cond.likelihood.re<- jags(data=cond.data ,inits=inits,
                              parameters.to.save =c("mu.p53", "sigma.p53"), 
                              model = cond.likelihood.re.model)
  }
  
  fixed <- jags(data=data$simdata, inits=inits, 
                parameters.to.save =c("pind", "mu.p53", "sigma.p53","mu.p53.notzero"), 
                model = fixed.model)
  original<- jags(data=data$simdata, inits=inits,
                  parameters.to.save =c("mu.p53", "sigma.p53"),
                  model = original.model)
  
  return(list(ones.cauchy = ones.cauchy, ones.normal=ones.normal,
              latent.normal = latent.normal,latent.cauchy = latent.cauchy,
              cond.likelihood = cond.likelihood,fixed = fixed, 
              original= original,cl.re=cond.likelihood.re, beta.p53 = data$beta.p53))
}

```


```{r parallel}
# stopCluster(cl)
cl = makeCluster(8)
registerDoParallel(cl)

# inits= list(mu.p53 = 0, mu.p53.notzero=0, phi.p53=1)
# inits$.RNG.name = "lecuyer::RngStream"
# inits$.RNG.seed = randomNumbers(n=1, min=1, max=1e+06, col=1)

mu=0.203;sd= 0.05831085 #from glmer estimates using all data
assoc<-c(0,1,1,1,1)
muvec<-c(0,mu,mu,mu,mu)
sdvec<-c(1,.5*sd,sd,2*sd,4*sd)

I = 10 #getDoParWorkers()*10
#I=1

system.time({
  allsim.u<- foreach(j = 1:5) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 7))
    }
})
save.image()

```

```{r}

#########
cl = makeCluster(8)
registerDoParallel(cl)
mu=0.203;sd= 0.05831085 #from glmer estimates using all data
assoc<-c(1,1)
muvec<-c(mu,mu)
sdvec<-c(sd,4*sd)

I = 10 #getDoParWorkers()*10
#I=1

system.time({
  allsim.30<- foreach(j = 1:2) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.all(assoc[j],muvec[j],sdvec[j], NULL, 30))
    }
})
save.image()
```


```{r run cl}
run.cl.better<- function(assoc,mu, sd, inits, pvals){ #slower but will use the same data so hopefully better
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd)
    cond.data<-get.cond.likelihood.data(data$simdata, min(pvals))
    count=count+1
  }
  if(!is.null(cond.data)){
    cond.likelihood<- lapply(pvals, function(p){
      cond.data<-get.cond.likelihood.data(data$simdata, p)
      return(jags(data=cond.data ,inits=inits,
                  parameters.to.save =c("pind", "mu.p53", "phi.p53"), 
                  model = cond.likelihood.model))
    })
  }
  return(cond.likelihood)
}

run.cl<- function(assoc,mu, sd, inits, p){
  cond.data = NULL;count=0
  while(is.null(cond.data) & count<1000){
    data <- sample(assoc, mu, sd)
    cond.data<-get.cond.likelihood.data(data$simdata, p)
    count=count+1
  }
  if(!is.null(cond.data)){
    cond.likelihood <- jags(data=cond.data ,inits=inits,
                            parameters.to.save =c("pind", "mu.p53", "sigma.p53"), 
                            model = cond.likelihood.model)
  }
  return(cond.likelihood)
}

```

```{r parallel cl}
cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
I=100
system.time({
  cl.sim<- foreach(j = 1:length(pvals)) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl(1,mu,sd, NULL,pvals[j]))
    }
})
save.image()

cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
I=100
system.time({
  cl.sim2<- foreach(j = 1:length(pvals)) %:%
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl(1,mu,sd*4, NULL,pvals[j]))
    }
})
save.image()


cl = makeCluster(8)
registerDoParallel(cl)
pvals=c(.05,.01,.005,.001, 1e-7)
I=100
system.time({
  cl.sim.better<-
    foreach ( i = 1:I, .inorder=FALSE, .packages = c("R2jags"))  %dopar% {
      return(run.cl.better(1,mu,sd*4, NULL,pvals))
    }
})
save.image()

```

```{r summary stats}
getstatsitems<-c("hasmu","haszero","mean","median",
                 "ismultimodal", "intervallength","probzero",
                 "hassd","sdmean","sdmedian","sdintlen",
                 "haspind","pindmean","pindmedian","pindintlen")

modelnames<- c("Cauchy-Ones Trick","Normal-Ones Trick",
               "Normal-Latent","Cauchy-Latent",
               "Cond Likelihood", "Fixed Effect", "Original","CL RE")
getstatsloop<-function(simlist,mu,sd,pind,modelnames, e=.03){
  I<-length(simlist)
  vec<-sapply(simlist, function(outputlist) sapply(outputlist[-length(outputlist)],getstats,mu,sd,pind, e))
  return(array(data=vec,dim= c(length(getstatsitems),length(modelnames),I), dimnames=list(getstatsitems,modelnames,seq(1:I))))
}

getstats<-function(jagsoutput, mu, sd, pind, e = .03){
  mu.samples<- jagsoutput$BUGSoutput$sims.list$mu.p53
  cred<-HPDM(mu.samples,e)
  upper<- cred[2]
  lower<- cred[1]
  if(length(cred)>2){
    #assume 2 modes max
    upper <- c(upper,cred[4])
    lower <- c(lower,cred[3])
  }
  sd.samples <- (jagsoutput$BUGSoutput$sims.list$sigma.p53)
  sd.cred<- HPDM(sd.samples)
  
  pind.samples<-pind.cred<-NA
  if(!is.null(jagsoutput$BUGSoutput$sims.list$pind)){
    pind.samples<- jagsoutput$BUGSoutput$sims.list$pind
    pind.cred<- HPDM(pind.samples)
  }
  return(c(hasmu = any(upper>=mu&lower<=mu), 
           haszero = any(upper>=-e&lower<=e), 
           mean = mean(mu.samples),
           median = median(mu.samples),
           ismultimodal=length(cred)>2,
           intervallength = sum(upper-lower),
           probzero = sum(abs(mu.samples)<=e)/length(mu.samples),
           hassd= sd.cred[2]>=sd&sd.cred[1]<=sd,
           sdmean= mean(sd.samples),
           sdmedian= median(sd.samples),
           sdintlen=sd.cred[2]-sd.cred[1],
           haspind=pind.cred[2]>=pind&pind.cred[1]<=pind,
           pindmean =mean(pind.samples),
           pindmedian=median(pind.samples),
           pindintlen=pind.cred[2]-pind.cred[1]
           
  ))
}

```


```{r get stats}
sim.0<-getstatsloop(allsim[[1]],0, 1, 0) #13 modes?
sim.half<-getstatsloop(allsim[[2]],mu, sd*.5, 1)
sim.1<-getstatsloop(allsim[[3]],mu, sd, 1)
sim.2<-getstatsloop(allsim[[4]],mu,sd*2,1)
sim.4<-getstatsloop(allsim.5[[1]],mu,sd*4,1)

```





```{r plotting fn}
make_simlist<-function(sim){
  return(lapply(seq(dim(sim)[2]), function(x) t(sim[ , x, ])))
}

make_estimate_error_plot<-function(simlist, means,medians, true,modelnames,plottitle = "Absolute Error"){
  mean.sqerr<- data.frame(sapply(simlist, function(x) abs(x[,means]-true)))
  colnames(mean.sqerr)<-modelnames
  
  median.sqerr<- data.frame(sapply(simlist, function(x) abs(x[,medians]-true)))
  colnames(median.sqerr)<-modelnames
  df <- data.frame(apply(mean.sqerr,2,function(x) sqrt(mean(x^2))),
                   apply(median.sqerr,2,function(x) sqrt(mean(x^2))))
  colnames(df)<-c("Posterior Mean", "Posterior Median")
  mean.sqerr$estimator <- rep("mean",dim(mean.sqerr)[1])
  median.sqerr$estimator <- rep("median",dim(median.sqerr)[1])
  
  sqerr<- rbind(mean.sqerr,median.sqerr)
  return(list(plot = ggplot(data = melt(sqerr), 
                            aes(x=variable, y=value,color=estimator)) +
                geom_boxplot()+labs(x= "Posterior Mean", y="Absolute Error",
                                    title= plottitle),
              table = df))
}

make_plots<-function(simlist, mu, sd, modelnames){
  #mse
  muerr<- (make_estimate_error_plot(simlist,3,4,mu,modelnames,"Mu Error Distribution"))
  
  #bar plots for coverage, times it's right
  coverage<- data.frame(sapply(simlist, function(x) x[,1]))
  colnames(coverage)<-modelnames
  
  containszero<- data.frame(sapply(simlist, function(x) x[,2]))
  both<- coverage*containszero
  coverage[coverage==1]<-2
  coverage[containszero==1]<-1
  coverage[both==1]<-3
  coverage1<-melt(coverage)
  levelnames<-c("Neither", "Only Zero","Only Mu","Both")
  coverage1$value<-factor(levelnames[coverage1$value+1])
  mucoverage= table(coverage1)/100.
  print(coverage1)
  
  # disjoint<- data.frame(sapply(simlist, function(x) x[,5]))
  # d0 = (containszero==0)*(disjoint)
  # colnames(d0) = modelnames
  # print(kable(table(melt(d0))))
  #interval length
  intlen<- data.frame(sapply(simlist, function(x) x[,6]))
  colnames(intlen)<-modelnames
  
  plot3<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "Intervals"))
  #prob of zero
  p0<- data.frame(sapply(simlist, function(x) x[,7]))
  colnames(p0)<-modelnames
  
  plot4 <- (ggplot(data = melt(p0), aes(x=variable, y=value)) +
              geom_boxplot()+labs(x= "Model", y="probability of mu=0", 
                                  title= "Posterior Probability of Null (H=0)"))
  
  ################sd, pind
  sderr<- (make_estimate_error_plot(simlist, 9,10,sd,modelnames,"Absolute Error of Standard Deviation"))
  
  pinderr<- (make_estimate_error_plot(simlist, 13,14,0,modelnames, "Absolute Error of Probability of H0"))
  
  intlen<- data.frame(sapply(simlist, function(x) x[,11]))
  colnames(intlen)<-modelnames
  plot7<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "SD Intervals"))
  intlen<- data.frame(sapply(simlist, function(x) x[,15]))
  colnames(intlen)<-modelnames
  plot8<-(ggplot(data = melt(intlen), aes(x=variable, y=value)) +
            geom_boxplot()+labs(x= "Model", y="Credible Interval Length", 
                                title= "pind Intervals"))
  sdcov<- data.frame(sapply(simlist, function(x) x[,8]))
  colnames(sdcov)<-modelnames
  sdcoverage<-table(melt(sdcov))/100.
  
  pindcov<- data.frame(sapply(simlist, function(x) x[,12]))
  colnames(pindcov)<-modelnames
  pindcoverage<-table(melt(pindcov))/100.
  
  return(list(plots = list(muerr$plot, plot3,plot4,sderr$plot,
                           pinderr$plot,plot7,plot8),
              tables = list(muerr$table, sderr$table, pinderr$table,
                            mucoverage, sdcoverage,pindcoverage)))
}

```


```{r}
plots0<-make_plots(make_simlist(sim.0), 0,1, modelnames)

```

```{r}
plots1<-make_plots(make_simlist(sim.1), mu,sd,modelnames[1:6])

```


```{r}
make_plots(make_simlist(sim.half), mu,sd, 1,modelnames)
make_plots(make_simlist(sim.2),mu,sd, 1,modelnames)
plots4<-make_plots(make_simlist(sim.4), mu,sd, 1,modelnames)

```

```{r cl analysis}
sim.cl.1<-sapply(cl.sim[[1]], getstats,mu,sd,1)
sim.cl.2<-sapply(cl.sim[[2]], getstats,mu,sd,1)
sim.cl.3<-sapply(cl.sim[[3]], getstats,mu,sd,1)
sim.cl.4<-sapply(cl.sim[[4]], getstats,mu,sd,1)
sim.cl.5<-sapply(cl.sim[[5]], getstats,mu,sd,1)
make_plots(list(t(sim.cl.1), t(sim.cl.2), t(sim.cl.3),t(sim.cl.4),t(sim.cl.5)), mu,sd, pvals)
#change this to lapply

sim.cl.stats2 <- lapply(cl.sim2, function(x) t(sapply(x, getstats,mu,sd,1)))

clplots2<-make_plots(sim.cl.stats2, mu, sd,1,pvals)

#return(array(data=vec,dim= c(length(getstatsitems),length(modelnames),I), dimnames=list(getstatsitems,modelnames,seq(1:I))))

vec <- sapply(cl.sim.better, function(x) sapply(x, getstats,mu,sd,1))
sim.cl.stats.better <-array(data=vec,dim= c(length(getstatsitems),length(pvals),I))
cl.better.simlist<-make_simlist(sim.cl.stats.better)
cl.better.plots<-make_plots(cl.better.simlist, mu,sd,pvals)
```

#### assorted notes/explanations/stuff- will move around once things are more set

Simulations: 


The data are generated from a hierarchical logistic model as follows: if truly associated, $\mu$ and $\sigma^2$ have (fixed) nonzero values;$\beta_j \sim N(\mu, \sigma^2)$. Otherwise, $\mu=\beta_j = 0,  \forall j$. The observed data $Y_{ij}$ is binary, and has $P(Y_{ij}=1| \beta_j) = \frac{e^{\beta_j}}{1+e^{\beta_j}}$. The $j$ index corresponds to the "group" to which the observation belongs.

To try to keep this simulation as close to the real data as possible, a preliminary logistic regression with random slope and random p53 coefficient by site was run. This led to the values of $\mu = , \sigma^2 = $ **insert values, link to freq analysis**. The value of $\mu$ remained fixed through all the simulations, but different values of $\sigma$ were used to test the sensitivity of the models: $\sigma$, $\sigma/2$, $2\sigma$, and  $4\sigma$. The number of sites was set to 7 and 30 to compare. Each site had 1000 observations.

The models are specified as follows: 


1. The probability of association $\xi$ can give rise to a latent variable drawn from a bernoulli, which is then used to parametrize the distribution of $\mu$. That is,  $\pi(\mu|\iota) = (1-\iota ) N(\mu, 0, \epsilon)+ \iota N(\mu,0 ,1)$, and $P(\iota = 1 ) = \xi$. This is the latent variable model with normal prior. Then $\xi \sim Beta(1/2, 1/2), \sigma \sim invGamma(1, .05), \beta_j \sim N(\mu, \sigma^2), Y_{ij} \sim logit^{-1}(\beta_j)$,
$.

2. The point mass can be approximated using a very concentrated normal distribution, centered at 0. Thus, $\pi(\mu|\xi) = (1-\xi) N(\mu, 0, \epsilon)+ \xi N(\mu,0 ,1)$. The other priors are the same as in the latent variable model.

Models that do not use the latent variable must specify the mixture distribution in JAGS directly. This is done through the ones trick, which use the Bernoulli distribution in order to allow for arbitrary distributions. Consider a prior  for $\theta$ that is proportional to $\pi(\theta)$. If we set that bernoulli variable "ones" is equal to 1 with probability $\pi(\theta)$, create an observation "ones"$= 1$, and set a uniform prior for $\theta$, then we are effectively creating a "posterior" for theta that is proportional to  $\pi(\theta)$ as intended.

In these scenarios, the model from which a sample comes from is also not evident. To determine whether a small value is truly from the concentrated normal, we can compute the probability of the sample for the two distributions and choose the model that results in a larger one.

3. and 4. The "ones-cauchy" and "latent-cauchy" have the same structure as the original ones, but use a $Cauchy(0,1)$ prior for $\mu$ to allow for larger effects without increasing the variance and making the prior too diffuse, which could lead to Lindley's paradox. In the case of the ones model, the point mass approximation is kept as a Normal.

5. The "fixed" model is just the same as the latent variable model with normal prior, but without the site effect.

6. The "original" model is a slightly modified version of the one used in the previous analysis of the TP53 data (see Data section **link or st here**). This has a normal prior for $\mu$, the global effect. This model is equivalent to the mixture model given the alternative (i.e. that mu is significant or $\xi = 1$).

7. and 8. The conditional likelihood model takes into account the probability of a "significant" event occurring when calculating its likelihood (i.e. the likelihood conditional on having significant data). In this model, we differentiate between the discovery and validation data. Given the discovery sites' MLE and SE, we can use the CLT and definition of MLE to state that $MLE_i \sim N(\mu, SE_i)$. Conditioning on the fact that this estimate is significant, $P(MLE_i) = \frac{\phi(MLE_i, \mu, SE_i)}{\Phi(-q, \mu, SE_i)+1-\Phi(q, \mu, SE_i)}$, where $\phi(x, \mu, \sigma)$ is the pdf of a normal distribution with mean $\mu$ and variance $\sigma^2$, and $\Phi(x, \mu, \sigma)$ is the cdf of the same distribution. The value of $q$ is $\Phi^{-1}(1-\frac{\alpha}{2}, 0 ,SE_i)$, and $\alpha$ is the power of the test (that is, p-values that are greater than $\alpha$ are not considered significant). Let this distribution be denoted as $CL(\mu,SE_i, q)$. The MLE and SE are sufficient statistics.

In this simulation study, a logistic regression with fixed effects for sites was conducted to find the site with the smallest p-value less than $\alpha$. If no sites matched this description, the data was resampled until at least one site was viable. The observations for this site were then taken out of the data, and the maximum likelihood estimate of this effect and its variance were added for the Bayesian model. The full specification of this model is as follows:
$Y_{ij} \sim logit^{-1}(\mu)$,
$MLE_k \sim CL(\mu,SE_k, q)$,
$\mu \sim N(0, .1)$.
The validation sites are modeled with the mixed effect model with a normal prior for $\mu$ and inverse gamma for $\sigma$.

There is one model specified with random effects and one without. The fixed effect model is described above, and the random effect model assumes $MLE_i$ comes from a normal distribution centered at site effect $\beta_i$ rather than at $\mu$, and that the validation data $Y_{ij} \sim N(\beta_j, \sigma)$ as in the fully Bayesian models.




##### Some Analysis

Sensitivity of CL
The conditional likelihood method is not robust to changes in the level $\alpha$. To test this, we consider 5 different levels: $0.05, 0.01, 0.005, 0.001, 10^{-7}$. 100 datasets were sampled, for which at least one location was significant at the smallest $\alpha$ level. The conditional likelihood model for each level was then fitted. The results from these show that, while the posterior mean estimate was the same for different $\alpha$, the credible intervals got larger as $\alpha$ decreased. This may seem counterintuitive, since we would expect that the lower $\alpha$ would yield more "precise" results, but this can be explained by the fact that the models take into account the selection mechanism. This means that, if an estimate has p-value much lower than $\alpha$, the model will be more confident that it came from a nonzero distribution than if it is close to $\alpha$.

One thing to note is that this only holds for distributions of $\beta$ that have small enough variance that there are no (or very few) sites are less than or equal to zero. Running the exact same simulation with the data coming from a more spread out distribution for $\mu$, we find that there is no difference between models. This is because the single significant site is overpowered by the rest of them, which are highly likely to not be significant because they are too spread out.


General model things
Mu
Bias/RMSE


### Sensitivity

As expected, the models performed more poorly as $\sigma$ increased. However, while one would expect the probability of being associated ($\xi$) to also increase with $\sigma$, this was not true for the implementations using the ones trick, indicating that **a**.


### Probability of H0 v H1

Simulated datasets that were not truly associated were also fit with the models described above, all of which did well as expected. **table**

### Number of Sites

Although using 7 sites more closely mimics the real data, these are not enough to get a good estimate if we consider each site as an observation of sorts in the hierarchical model. Thus, to test the behaviour of the priors for $\sigma$, this number was increased to 30, which led to significantly improved estimates for sigma, showing that although the inverse gamma was not the best fit, part of the reason it led to overestimates of theta was the small number of sites.


### mu and zero part
JAGS of latent v ones trick- potentially have some traceplots to show
 

One problem that arises when using the ones trick is that the markov chain does not sample from the entire space, but rather gets stuck within a subset of values **figure this out**. This leads to posteriors for $\mu$ that are either positive or negative, with almost no zero values.


### sigma

Both the uniform and the truncated cauchy lead to better estimates of the variance than the inverse gamma on simulated datasets with 7 sites and small true variance. The cauchy prior estimates have smaller RMSE than the uniform prior for the mixed models using latent variables, the original normal model, and the random effect (although the difference is small). The uniform produces better posterior mean estimates for the ones trick specifications. However, regardless of the prior used, the latent variable models are still closer to the true values.





